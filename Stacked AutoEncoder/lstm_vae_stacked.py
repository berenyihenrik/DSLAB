# -*- coding: utf-8 -*-
"""LSTM VAE stacked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dtdnekLmiEz_jbu1w-bsk7ZQeICH1Rpg

# Setup Environment and Read Data
"""

import torch
import numpy as np
import pandas as pd
import pickle
import copy
import time
from tqdm import trange,tqdm
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score
from torch.cuda.amp import autocast, GradScaler

"""## Setup the dataset"""

DRIVE = "drive/MyDrive/Colab Notebooks/ELTE/DSLAB/ServerMachineDataset/"
DRIVE = "/mnt/c/Users/beren/Desktop/DSLAB/datasets/ServerMachineDataset/"
MACHINE = "machine-1-1.txt"
TRAIN_DATASET = DRIVE + "train/" + MACHINE
TEST_DATASET = DRIVE + "test/" + MACHINE
TEST_LABEL_DATASET = DRIVE + "test_label/" + MACHINE

metric = pd.read_csv(TRAIN_DATASET, header=None)
metric_test = pd.read_csv(TEST_DATASET, header=None)
true_anomalies = pd.read_csv(TEST_LABEL_DATASET, header=None)[0].to_numpy()

metric

"""### Non-Scaled"""

# create train and test dataloaders
metric.interpolate(inplace=True)
metric.bfill(inplace=True)
metric_tensor = metric.values

metric_test.interpolate(inplace=True)
metric_test.bfill(inplace=True)
metric_test_tensor = metric_test.values

sequence_length = 30
sequences = []
for i in range(metric_tensor.shape[0] - sequence_length + 1):
  sequences.append(metric_tensor[i:i + sequence_length])


train_data, val_data = train_test_split(sequences, test_size=0.3, random_state=42) # 70% train, 30% validation

test_sequences = []
for i in range(metric_test_tensor.shape[0] - sequence_length + 1):
  test_sequences.append(metric_test_tensor[i:i + sequence_length])


batch_size = 128
train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(dataset=test_sequences, batch_size=batch_size, shuffle=False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

sequences[0].shape

"""## Stacked"""

class LSTMEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):
        super(LSTMEncoder, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=False)
        self.fc_mean = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

    def forward(self, x):
        # x can be (batch, seq, input_dim) for single sensor or batched sensors
        _, (h_n, _) = self.lstm(x)  # h_n: (num_layers, batch, hidden_dim)
        h = h_n[-1]  # take the output of the last layer
        return self.fc_mean(h), self.fc_logvar(h)


class SharedDecoder(nn.Module):
    def __init__(self, input_features_dim, hidden_dim, output_features_dim, sequence_length, num_layers=1):
        super(SharedDecoder, self).__init__()
        self.sequence_length = sequence_length
        # The input to the decoder's linear layer is the concatenated latent vector
        self.latent_to_hidden = nn.Linear(input_features_dim, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        # The output of the decoder's linear layer should be the reconstructed features for all sensors
        self.output_layer = nn.Linear(hidden_dim, output_features_dim)


    def forward(self, z):
        # Repeat z for each timestep
        hidden = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)
        out, _ = self.lstm(hidden)
        return self.output_layer(out)


class LSTMVAE_Stacked(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, sequence_length, num_layers=1, device='cpu', num_sensors=38):
        super(LSTMVAE_Stacked, self).__init__()
        self.input_dim = input_dim
        self.num_sensors = num_sensors
        self.sequence_length = sequence_length
        self.device = device
        
        # Keep separate encoders for interpretability
        self.encoders = nn.ModuleList([
            LSTMEncoder(input_dim, hidden_dim, latent_dim, num_layers).to(device) for _ in range(num_sensors)
        ])
        
        decoder_input_features = num_sensors * latent_dim
        decoder_output_features = input_dim * num_sensors
        self.decoder = SharedDecoder(decoder_input_features, hidden_dim, decoder_output_features, sequence_length, num_layers).to(device)

    def reparameterize(self, mean, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(self, x):
        # x shape: (batch_size, sequence_length, num_sensors * input_dim)
        batch_size = x.shape[0]
        
        # Reshape and permute to get (batch_size, num_sensors, seq_len, input_dim)
        x_reshaped = x.view(batch_size, self.sequence_length, self.num_sensors, self.input_dim)
        x_reshaped = x_reshaped.permute(0, 2, 1, 3)
        
        # Stack all sensor data: (batch_size * num_sensors, seq_len, input_dim)
        x_flat = x_reshaped.reshape(batch_size * self.num_sensors, self.sequence_length, self.input_dim)
        
        # Process all encoders - PyTorch will optimize this internally
        means = []
        logvars = []
        for i, encoder in enumerate(self.encoders):
            # Extract batch for this encoder
            x_sensor = x_flat[i::self.num_sensors]  # Every num_sensors-th element starting from i
            mean, logvar = encoder(x_sensor)
            means.append(mean)
            logvars.append(logvar)
        
        # Stack results
        mean_stacked = torch.stack(means, dim=1)  # (batch_size, num_sensors, latent_dim)
        logvar_stacked = torch.stack(logvars, dim=1)
        
        # Vectorized reparameterization
        z_stacked = self.reparameterize(mean_stacked, logvar_stacked)
        
        # Flatten for decoder
        mean_combined = mean_stacked.reshape(batch_size, -1)
        logvar_combined = logvar_stacked.reshape(batch_size, -1)
        z_combined = z_stacked.reshape(batch_size, -1)

        x_recon = self.decoder(z_combined)

        return x_recon, mean_combined, logvar_combined

num_sensors = 38
input_dim = 1
hidden_dim = 128
latent_dim = 32
num_layers = 1

model = LSTMVAE_Stacked(num_sensors=num_sensors,
                input_dim=input_dim,
                hidden_dim=hidden_dim,
                latent_dim=latent_dim,
                sequence_length=sequence_length,
                num_layers=num_layers,
                device=device).to(device)
optimizer = Adam(model.parameters(), lr=1e-3)

batch = [torch.randn(batch_size, sequence_length, 1) for _ in range(num_sensors)]

# Concatenate the list of tensors into a single tensor
batch_tensor = torch.cat(batch, dim=2).to(device) # Concatenate along the last dimension (features)

output, _, _ = model(batch_tensor)  # [batch_size, seq_len, num_sensors * input_dim]
print(output.shape)

"""## Support functions"""

def loss_function(x, x_hat, mean, log_var):
    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')
    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())

    return reproduction_loss + KLD

def save_model(model, name):
    model_state = {
        'input_dim':input_dim,
        'latent_dim':latent_dim,
        'hidden_dim':hidden_dim,
        'state_dict':model.state_dict()
    }
    torch.save(model_state, name + '.pth')

"""# Train

## LSTM
"""

torch.cuda.empty_cache()

scaler = GradScaler()
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)

# SPO optimizer - optuna
# bayesian hyperparameter tuning
# grid search - slow for DL

def train_model(model, train_loader, val_loader, optimizer, loss_fn, scheduler, num_epochs=10, device='cpu'):
    torch.cuda.empty_cache()
    train_losses = []
    val_losses = []

    early_stop_tolerant_count = 0
    early_stop_tolerant = 10
    best_loss = float('inf')
    best_model_wts = copy.deepcopy(model.state_dict())

    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        train_loss = 0.0
        model.train()
        
        # Profiling timers
        forward_time = 0.0
        backward_time = 0.0
        
        for batch in train_loader:
            batch = torch.tensor(batch, dtype=torch.float32).to(device)
            optimizer.zero_grad()

            # Time forward pass
            t0 = time.time()
            # with autocast():
            recon_batch, mean, logvar = model(batch)
            loss = loss_fn(recon_batch, batch, mean, logvar)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            forward_time += time.time() - t0

            # Time backward pass
            t0 = time.time()
            # scaler.scale(loss).backward()
            # scaler.step(optimizer)
            # scaler.update()
            loss.backward()
            optimizer.step()

            if device.type == 'cuda':
                torch.cuda.synchronize()
            backward_time += time.time() - t0
            
            train_loss += loss.item()

        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        # Validation
        model.eval()
        valid_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                batch = torch.tensor(batch, dtype=torch.float32).to(device)
                recon_batch, mean, logvar = model(batch)
                loss = loss_fn(recon_batch, batch, mean, logvar)
                valid_loss += loss.item()

        valid_loss /= len(val_loader)
        val_losses.append(valid_loss)

        scheduler.step(valid_loss)

        epoch_time = time.time() - epoch_start_time

        if valid_loss < best_loss:
            best_loss = valid_loss
            best_model_wts = copy.deepcopy(model.state_dict())
            early_stop_tolerant_count = 0
        else:
            early_stop_tolerant_count += 1

        print(f"Epoch {epoch+1:04d}: train loss {train_loss:.4f}, valid loss {valid_loss:.4f}, "
              f"time {epoch_time:.2f}s (forward: {forward_time:.2f}s, backward: {backward_time:.2f}s)")

        if early_stop_tolerant_count >= early_stop_tolerant:
            print("Early stopping triggered.")
            break

    model.load_state_dict(best_model_wts)
    print("Finished Training.")
    return train_losses, val_losses

train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, loss_function, scheduler, num_epochs=100, device=device)

save_model(model, 'vae_stacked')

"""# Evaluate"""

def evaluate_lstm(model, test_loader, device, percentile_threshold=90):
    model.eval()
    anomaly_scores = []

    with torch.no_grad():
        for batch in test_loader:
            batch = torch.tensor(batch, dtype=torch.float32).to(device)

            batch_scores = []
            for i in range(batch.shape[0]): #Iterate through each sequence in the batch
                sequence = batch[i, :, :].unsqueeze(0)  # Select a single sequence
                recon_batch, mean, logvar = model(sequence)
                loss = loss_function(recon_batch, sequence, mean, logvar)
                batch_scores.append(loss.item())
            anomaly_scores.extend(batch_scores)  # Append scores for all sequences in the batch


    # Calculate the threshold based on the specified percentile
    threshold = np.percentile(anomaly_scores, percentile_threshold)

    # Identify anomaly indices
    anomaly_indices = [i for i, score in enumerate(anomaly_scores) if score > threshold]
    return anomaly_indices
anomalies = evaluate_lstm(model, test_loader, device, 90)

def calculate_f1_score(anomaly_indices, true_anomalies):
    # Create a binary array representing predicted anomalies
    predicted_anomalies = np.zeros_like(true_anomalies)
    for index in anomaly_indices:
        if index < len(predicted_anomalies):  # Check index bounds
          predicted_anomalies[index] = 1

    # Calculate the F1 score
    f1 = f1_score(true_anomalies, predicted_anomalies)
    return f1, predicted_anomalies

# Example usage (assuming 'anomalies' and 'true_anomalies' are defined)
f1, predicted_anomalies = calculate_f1_score(anomalies, true_anomalies)
print(f"F1 Score: {f1}")

# Calculate AUC-ROC score
auc_roc = roc_auc_score(true_anomalies, predicted_anomalies)
print(f"AUC-ROC Score: {auc_roc}")

print(classification_report(true_anomalies, predicted_anomalies))

print(confusion_matrix(true_anomalies, predicted_anomalies))