# -*- coding: utf-8 -*-
"""LSTM VAE stacked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dtdnekLmiEz_jbu1w-bsk7ZQeICH1Rpg

# Setup Environment and Read Data
"""

import torch
import numpy as np
import pandas as pd
import pickle
import copy
import time
from tqdm import trange,tqdm
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score
from torch.cuda.amp import autocast, GradScaler

"""## Setup the dataset"""

DRIVE = "drive/MyDrive/Colab Notebooks/ELTE/DSLAB/ServerMachineDataset/"
DRIVE = "/mnt/c/Users/beren/Desktop/DSLAB/datasets/ServerMachineDataset/"
MACHINE = "machine-1-1.txt"
TRAIN_DATASET = DRIVE + "train/" + MACHINE
TEST_DATASET = DRIVE + "test/" + MACHINE
TEST_LABEL_DATASET = DRIVE + "test_label/" + MACHINE

metric = pd.read_csv(TRAIN_DATASET, header=None)
metric_test = pd.read_csv(TEST_DATASET, header=None)
true_anomalies = pd.read_csv(TEST_LABEL_DATASET, header=None)[0].to_numpy()

metric

"""### Non-Scaled"""

# create train and test dataloaders
metric.interpolate(inplace=True)
metric.bfill(inplace=True)
metric_tensor = metric.values

metric_test.interpolate(inplace=True)
metric_test.bfill(inplace=True)
metric_test_tensor = metric_test.values

sequence_length = 30
sequences = []
for i in range(metric_tensor.shape[0] - sequence_length + 1):
  sequences.append(metric_tensor[i:i + sequence_length])


train_data, val_data = train_test_split(sequences, test_size=0.3, random_state=42) # 70% train, 30% validation

test_sequences = []
for i in range(metric_test_tensor.shape[0] - sequence_length + 1):
  test_sequences.append(metric_test_tensor[i:i + sequence_length])


batch_size = 128
train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(dataset=test_sequences, batch_size=batch_size, shuffle=False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

sequences[0].shape

"""Apply Feature Selection and Rebuild Dataloaders"""
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import numpy as np

# Prepare data for RandomForestClassifier using the test data and true anomalies
# We will use the test data (metric_test_tensor) and the true anomaly labels (true_anomalies)
# to train the RandomForestClassifier for feature importance calculation.

# Flatten the test sequences for the RandomForestClassifier
n_samples_test, n_timesteps, n_features = np.array(test_sequences).shape
X_test_flat = np.array(test_sequences).reshape(n_samples_test, n_timesteps * n_features)
y_test_true = true_anomalies[:n_samples_test] # Use the true anomalies as the target

# Initialize and train the RandomForestClassifier
# We use a simple setup for demonstration. Hyperparameter tuning might be needed for optimal results.
# The goal here is not to build a perfect anomaly detection model with RF, but to get feature importances.
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_test_flat, y_test_true)

# Get feature importances
feature_importances = rf_model.feature_importances_

# The feature importances are for the flattened features (timesteps * features).
# We need to map these back to the original features.
# We can average the importance scores for each original feature across all timesteps.
original_feature_importances = feature_importances.reshape(n_timesteps, n_features).mean(axis=0)

# Rank features by importance
ranked_features_indices = np.argsort(original_feature_importances)[::-1]
ranked_features_importance = original_feature_importances[ranked_features_indices]

print("Feature Importances (averaged across timesteps):")
for i, index in enumerate(ranked_features_indices):
    print(f"Feature {index}: {ranked_features_importance[i]:.4f}")

# Determine the number of top features to select (e.g., top 20)
num_selected_features = 25 # You can adjust this number

# Get the indices of the top N features
selected_feature_indices = ranked_features_indices[:num_selected_features]
remaining_feature_indices = ranked_features_indices[num_selected_features:]
print(f"\nSelected Top {num_selected_features} Feature Indices: {selected_feature_indices}")
print(f"\nRemaining {len(remaining_feature_indices)} Feature Indices: {remaining_feature_indices}")

# Select only the top features for the datasets
metric_tensor_selected = metric_tensor[:, selected_feature_indices]
metric_test_tensor_selected = metric_test_tensor[:, selected_feature_indices]

# Create datasets with top features and remaining features
metric_tensor_top = metric_tensor[:, selected_feature_indices]
metric_tensor_remaining = metric_tensor[:, remaining_feature_indices]

metric_test_tensor_top = metric_test_tensor[:, selected_feature_indices]
metric_test_tensor_remaining = metric_test_tensor[:, remaining_feature_indices]

# Create sequences for both feature groups
sequence_length = 30
sequences_top = []
sequences_remaining = []
for i in range(metric_tensor.shape[0] - sequence_length + 1):
    sequences_top.append(metric_tensor_top[i:i + sequence_length])
    sequences_remaining.append(metric_tensor_remaining[i:i + sequence_length])

# Combine into tuples for dataloaders
sequences_combined = list(zip(sequences_top, sequences_remaining))
train_data_combined, val_data_combined = train_test_split(sequences_combined, test_size=0.3, random_state=42)

test_sequences_top = []
test_sequences_remaining = []
for i in range(metric_test_tensor.shape[0] - sequence_length + 1):
    test_sequences_top.append(metric_test_tensor_top[i:i + sequence_length])
    test_sequences_remaining.append(metric_test_tensor_remaining[i:i + sequence_length])

test_sequences_combined = list(zip(test_sequences_top, test_sequences_remaining))

batch_size = 32
train_loader_combined = DataLoader(dataset=train_data_combined, batch_size=batch_size, shuffle=True)
val_loader_combined = DataLoader(dataset=val_data_combined, batch_size=batch_size, shuffle=False)
test_loader_combined = DataLoader(dataset=test_sequences_combined, batch_size=batch_size, shuffle=False)

print(f"Top features dimension: {len(selected_feature_indices)}")
print(f"Remaining features dimension: {len(remaining_feature_indices)}")

"""## Stacked with Weighted Encoders"""

class LSTMEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):
        super(LSTMEncoder, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=False)
        self.fc_mean = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        h = h_n[-1]
        return self.fc_mean(h), self.fc_logvar(h)


class SharedDecoder(nn.Module):
    def __init__(self, input_features_dim, hidden_dim, output_features_dim_top, output_features_dim_remaining, sequence_length, num_layers=1):
        super(SharedDecoder, self).__init__()
        self.sequence_length = sequence_length
        self.latent_to_hidden = nn.Linear(input_features_dim, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        
        # Separate output layers for top and remaining features
        self.output_layer_top = nn.Linear(hidden_dim, output_features_dim_top)
        self.output_layer_remaining = nn.Linear(hidden_dim, output_features_dim_remaining)

    def forward(self, z):
        hidden = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)
        out, _ = self.lstm(hidden)
        recon_top = self.output_layer_top(out)
        recon_remaining = self.output_layer_remaining(out)
        return recon_top, recon_remaining


class LSTMVAE_Stacked_Weighted(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, sequence_length, num_layers=1, 
                 device='cpu', num_top_sensors=25, num_remaining_sensors=13, 
                 top_weight=0.7, remaining_weight=0.3):
        super(LSTMVAE_Stacked_Weighted, self).__init__()
        self.input_dim = input_dim
        self.num_top_sensors = num_top_sensors
        self.num_remaining_sensors = num_remaining_sensors
        self.sequence_length = sequence_length
        self.device = device
        self.top_weight = top_weight
        self.remaining_weight = remaining_weight
        
        # Separate encoders for top features
        self.encoders_top = nn.ModuleList([
            LSTMEncoder(input_dim, hidden_dim, latent_dim, num_layers).to(device) 
            for _ in range(num_top_sensors)
        ])
        
        # Single encoder for remaining features (processes all at once)
        self.encoder_remaining = LSTMEncoder(num_remaining_sensors * input_dim, hidden_dim, latent_dim, num_layers).to(device)
        
        # Decoder input is concatenation of all latent representations
        decoder_input_features = (num_top_sensors + 1) * latent_dim
        decoder_output_features_top = input_dim * num_top_sensors
        decoder_output_features_remaining = input_dim * num_remaining_sensors
        
        self.decoder = SharedDecoder(
            decoder_input_features, 
            hidden_dim, 
            decoder_output_features_top,
            decoder_output_features_remaining,
            sequence_length, 
            num_layers
        ).to(device)

    def reparameterize(self, mean, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(self, x_top, x_remaining):
        # x_top shape: (batch_size, sequence_length, num_top_sensors)
        # x_remaining shape: (batch_size, sequence_length, num_remaining_sensors)
        batch_size = x_top.shape[0]
        
        # Process top features with individual encoders
        x_top_reshaped = x_top.view(batch_size, self.sequence_length, self.num_top_sensors, self.input_dim)
        x_top_reshaped = x_top_reshaped.permute(0, 2, 1, 3)
        x_top_flat = x_top_reshaped.reshape(batch_size * self.num_top_sensors, self.sequence_length, self.input_dim)
        
        means_top = []
        logvars_top = []
        for i, encoder in enumerate(self.encoders_top):
            x_sensor = x_top_flat[i::self.num_top_sensors]
            mean, logvar = encoder(x_sensor)
            means_top.append(mean)
            logvars_top.append(logvar)
        
        mean_top_stacked = torch.stack(means_top, dim=1)
        logvar_top_stacked = torch.stack(logvars_top, dim=1)
        z_top_stacked = self.reparameterize(mean_top_stacked, logvar_top_stacked)
        
        # Process remaining features with single encoder
        x_remaining_flat = x_remaining.view(batch_size, self.sequence_length, -1)
        mean_remaining, logvar_remaining = self.encoder_remaining(x_remaining_flat)
        z_remaining = self.reparameterize(mean_remaining, logvar_remaining)
        
        # Combine latent representations
        z_top_combined = z_top_stacked.reshape(batch_size, -1)
        z_combined = torch.cat([z_top_combined, z_remaining], dim=1)
        
        mean_combined = torch.cat([mean_top_stacked.reshape(batch_size, -1), mean_remaining], dim=1)
        logvar_combined = torch.cat([logvar_top_stacked.reshape(batch_size, -1), logvar_remaining], dim=1)
        
        # Decode
        x_recon_top, x_recon_remaining = self.decoder(z_combined)
        
        return x_recon_top, x_recon_remaining, mean_combined, logvar_combined

num_top_sensors = len(selected_feature_indices)
num_remaining_sensors = len(remaining_feature_indices)
input_dim = 1
hidden_dim = 128
latent_dim = 32
num_layers = 1

"""## Support functions"""

def loss_function_weighted(x_top, x_remaining, x_hat_top, x_hat_remaining, mean, log_var, top_weight=0.7, remaining_weight=0.3):
    reproduction_loss_top = nn.functional.mse_loss(x_hat_top, x_top, reduction='sum')
    reproduction_loss_remaining = nn.functional.mse_loss(x_hat_remaining, x_remaining, reduction='sum')
    
    # Weighted reconstruction loss
    reproduction_loss = top_weight * reproduction_loss_top + remaining_weight * reproduction_loss_remaining
    
    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
    
    return reproduction_loss + KLD

def load_model(name, device):
    checkpoint = torch.load(name + '.pth', map_location=device)
    model = LSTMVAE_Stacked_Weighted(
        input_dim=checkpoint['input_dim'],
        hidden_dim=checkpoint['hidden_dim'],
        latent_dim=checkpoint['latent_dim'],
        sequence_length=sequence_length,
        num_layers=num_layers,
        device=device,
        num_top_sensors=num_top_sensors,
        num_remaining_sensors=num_remaining_sensors
    ).to(device)
    model.load_state_dict(checkpoint['state_dict'])
    return model

model = load_model('vae_stacked_weighted', device)

"""# Evaluate"""

def evaluate_lstm_weighted(model, test_loader, device, percentile_threshold=90):
    model.eval()
    anomaly_scores = []

    with torch.no_grad():
        for batch_top, batch_remaining in test_loader:
            batch_top = torch.tensor(batch_top, dtype=torch.float32).to(device)
            batch_remaining = torch.tensor(batch_remaining, dtype=torch.float32).to(device)

            batch_scores = []
            for i in range(batch_top.shape[0]):
                sequence_top = batch_top[i, :, :].unsqueeze(0)
                sequence_remaining = batch_remaining[i, :, :].unsqueeze(0)
                
                recon_top, recon_remaining, mean, logvar = model(sequence_top, sequence_remaining)
                loss = loss_function_weighted(sequence_top, sequence_remaining, recon_top, recon_remaining, 
                                             mean, logvar, model.top_weight, model.remaining_weight)
                batch_scores.append(loss.item())
            anomaly_scores.extend(batch_scores)

    threshold = np.percentile(anomaly_scores, percentile_threshold)
    anomaly_indices = [i for i, score in enumerate(anomaly_scores) if score > threshold]
    return anomaly_indices

anomalies = evaluate_lstm_weighted(model, test_loader_combined, device, 90)

def calculate_f1_score(anomaly_indices, true_anomalies):
    # Create a binary array representing predicted anomalies
    predicted_anomalies = np.zeros_like(true_anomalies)
    for index in anomaly_indices:
        if index < len(predicted_anomalies):  # Check index bounds
          predicted_anomalies[index] = 1

    # Calculate the F1 score
    f1 = f1_score(true_anomalies, predicted_anomalies)
    return f1, predicted_anomalies

f1, predicted_anomalies = calculate_f1_score(anomalies, true_anomalies)
print(f"F1 Score: {f1}")

auc_roc = roc_auc_score(true_anomalies, predicted_anomalies)
print(f"AUC-ROC Score: {auc_roc}")

auc_pr = average_precision_score(true_anomalies, predicted_anomalies)
print(f"AUCPR Score: {auc_pr}")

print(classification_report(true_anomalies, predicted_anomalies))
print(confusion_matrix(true_anomalies, predicted_anomalies))