{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1sOd7lFAtYGu9m3QHlYPyXVgWZ_R9YKph","authorship_tag":"ABX9TyMSoliGayDil/7R82p6PCFg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup Environment and Read Data"],"metadata":{"id":"S0GM4bxG5gHo"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import copy\n","from tqdm import trange,tqdm\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix"],"metadata":{"id":"zhjtQGemJkY-","executionInfo":{"status":"ok","timestamp":1759671011878,"user_tz":-120,"elapsed":3063,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Download the dataset"],"metadata":{"id":"K-KbrFb3E8oM"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGz5Bo6KqUni","executionInfo":{"status":"ok","timestamp":1745244244036,"user_tz":-120,"elapsed":596,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"86ac78ee-e36a-4f22-b646-a78e9af52a03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-21 14:04:03--  https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3956 (3.9K) [text/plain]\n","Saving to: ‘labeled_anomalies.csv’\n","\n","labeled_anomalies.c 100%[===================>]   3.86K  --.-KB/s    in 0s      \n","\n","2025-04-21 14:04:04 (32.3 MB/s) - ‘labeled_anomalies.csv’ saved [3956/3956]\n","\n"]}]},{"cell_type":"code","source":["%env DRIVE_PATH=/content/drive/MyDrive/Colab Notebooks/ELTE/DSLAB/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNvs27uDrhhv","executionInfo":{"status":"ok","timestamp":1745244599689,"user_tz":-120,"elapsed":14,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"a5ed7cb7-5f53-4e68-99ef-34de9b01126d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: DRIVE_PATH=/content/drive/MyDrive/Colab Notebooks/ELTE/DSLAB/\n"]}]},{"cell_type":"code","source":["!mkdir \"/root/.config/kaggle\""],"metadata":{"id":"0DaTKJv8rhvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp \"$DRIVE_PATH/kaggle.json\" \"/root/.config/kaggle\"\n","!chmod 600 \"/root/.config/kaggle/kaggle.json\""],"metadata":{"id":"zU_B0rU3uY6o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cd \"$DRIVE_PATH\" && kaggle datasets download -d patrickfleith/nasa-anomaly-detection-dataset-smap-msl && mv nasa-anomaly-detection-dataset-smap-msl.zip data.zip && unzip -o data.zip && rm data.zip && mv data/data tmp && rm -r data && mv tmp data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"A6Canu_LrNQy","executionInfo":{"status":"ok","timestamp":1745245305096,"user_tz":-120,"elapsed":14229,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"f519052d-682a-4771-fd41-0dfdc00912f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/patrickfleith/nasa-anomaly-detection-dataset-smap-msl\n","License(s): copyright-authors\n","Archive:  data.zip\n","  inflating: data/data/2018-05-19_15.00.10/models/A-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-5.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-6.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-8.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/A-9.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/B-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/C-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/C-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-11.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-12.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-13.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-14.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-15.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-16.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-5.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-6.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-8.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/D-9.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-10.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-11.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-12.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-13.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-5.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-6.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-8.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/E-9.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-5.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/F-8.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/G-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/G-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/G-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/G-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/G-6.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/G-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-5.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-6.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/M-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-10.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-11.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-14.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-15.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/P-7.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/R-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/S-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/S-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-1.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-10.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-12.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-13.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-2.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-3.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-4.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-5.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-8.h5  \n","  inflating: data/data/2018-05-19_15.00.10/models/T-9.h5  \n","  inflating: data/data/2018-05-19_15.00.10/params.log  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/A-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/B-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/C-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/C-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-11.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-12.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-13.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-14.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-15.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-16.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/D-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-10.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-11.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-12.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-13.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/E-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/F-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/G-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/G-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/G-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/G-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/G-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/G-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/M-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-10.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-11.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-14.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-15.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/P-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/R-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/S-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/S-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-10.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-12.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-13.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/smoothed_errors/T-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/A-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/B-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/C-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/C-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-11.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-12.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-13.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-14.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-15.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-16.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/D-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-10.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-11.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-12.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-13.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/E-9.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/F-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/G-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/G-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/G-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/G-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/G-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/G-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-6.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/M-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-10.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-11.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-14.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-15.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/P-7.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/R-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/S-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/S-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-1.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-10.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-12.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-13.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-2.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-3.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-4.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-5.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-8.npy  \n","  inflating: data/data/2018-05-19_15.00.10/y_hat/T-9.npy  \n","  inflating: data/data/test/A-1.npy  \n","  inflating: data/data/test/A-2.npy  \n","  inflating: data/data/test/A-3.npy  \n","  inflating: data/data/test/A-4.npy  \n","  inflating: data/data/test/A-5.npy  \n","  inflating: data/data/test/A-6.npy  \n","  inflating: data/data/test/A-7.npy  \n","  inflating: data/data/test/A-8.npy  \n","  inflating: data/data/test/A-9.npy  \n","  inflating: data/data/test/B-1.npy  \n","  inflating: data/data/test/C-1.npy  \n","  inflating: data/data/test/C-2.npy  \n","  inflating: data/data/test/D-1.npy  \n","  inflating: data/data/test/D-11.npy  \n","  inflating: data/data/test/D-12.npy  \n","  inflating: data/data/test/D-13.npy  \n","  inflating: data/data/test/D-14.npy  \n","  inflating: data/data/test/D-15.npy  \n","  inflating: data/data/test/D-16.npy  \n","  inflating: data/data/test/D-2.npy  \n","  inflating: data/data/test/D-3.npy  \n","  inflating: data/data/test/D-4.npy  \n","  inflating: data/data/test/D-5.npy  \n","  inflating: data/data/test/D-6.npy  \n","  inflating: data/data/test/D-7.npy  \n","  inflating: data/data/test/D-8.npy  \n","  inflating: data/data/test/D-9.npy  \n","  inflating: data/data/test/E-1.npy  \n","  inflating: data/data/test/E-10.npy  \n","  inflating: data/data/test/E-11.npy  \n","  inflating: data/data/test/E-12.npy  \n","  inflating: data/data/test/E-13.npy  \n","  inflating: data/data/test/E-2.npy  \n","  inflating: data/data/test/E-3.npy  \n","  inflating: data/data/test/E-4.npy  \n","  inflating: data/data/test/E-5.npy  \n","  inflating: data/data/test/E-6.npy  \n","  inflating: data/data/test/E-7.npy  \n","  inflating: data/data/test/E-8.npy  \n","  inflating: data/data/test/E-9.npy  \n","  inflating: data/data/test/F-1.npy  \n","  inflating: data/data/test/F-2.npy  \n","  inflating: data/data/test/F-3.npy  \n","  inflating: data/data/test/F-4.npy  \n","  inflating: data/data/test/F-5.npy  \n","  inflating: data/data/test/F-7.npy  \n","  inflating: data/data/test/F-8.npy  \n","  inflating: data/data/test/G-1.npy  \n","  inflating: data/data/test/G-2.npy  \n","  inflating: data/data/test/G-3.npy  \n","  inflating: data/data/test/G-4.npy  \n","  inflating: data/data/test/G-6.npy  \n","  inflating: data/data/test/G-7.npy  \n","  inflating: data/data/test/M-1.npy  \n","  inflating: data/data/test/M-2.npy  \n","  inflating: data/data/test/M-3.npy  \n","  inflating: data/data/test/M-4.npy  \n","  inflating: data/data/test/M-5.npy  \n","  inflating: data/data/test/M-6.npy  \n","  inflating: data/data/test/M-7.npy  \n","  inflating: data/data/test/P-1.npy  \n","  inflating: data/data/test/P-10.npy  \n","  inflating: data/data/test/P-11.npy  \n","  inflating: data/data/test/P-14.npy  \n","  inflating: data/data/test/P-15.npy  \n","  inflating: data/data/test/P-2.npy  \n","  inflating: data/data/test/P-3.npy  \n","  inflating: data/data/test/P-4.npy  \n","  inflating: data/data/test/P-7.npy  \n","  inflating: data/data/test/R-1.npy  \n","  inflating: data/data/test/S-1.npy  \n","  inflating: data/data/test/S-2.npy  \n","  inflating: data/data/test/T-1.npy  \n","  inflating: data/data/test/T-10.npy  \n","  inflating: data/data/test/T-12.npy  \n","  inflating: data/data/test/T-13.npy  \n","  inflating: data/data/test/T-2.npy  \n","  inflating: data/data/test/T-3.npy  \n","  inflating: data/data/test/T-4.npy  \n","  inflating: data/data/test/T-5.npy  \n","  inflating: data/data/test/T-8.npy  \n","  inflating: data/data/test/T-9.npy  \n","  inflating: data/data/train/A-1.npy  \n","  inflating: data/data/train/A-2.npy  \n","  inflating: data/data/train/A-3.npy  \n","  inflating: data/data/train/A-4.npy  \n","  inflating: data/data/train/A-5.npy  \n","  inflating: data/data/train/A-6.npy  \n","  inflating: data/data/train/A-7.npy  \n","  inflating: data/data/train/A-8.npy  \n","  inflating: data/data/train/A-9.npy  \n","  inflating: data/data/train/B-1.npy  \n","  inflating: data/data/train/C-1.npy  \n","  inflating: data/data/train/C-2.npy  \n","  inflating: data/data/train/D-1.npy  \n","  inflating: data/data/train/D-11.npy  \n","  inflating: data/data/train/D-12.npy  \n","  inflating: data/data/train/D-13.npy  \n","  inflating: data/data/train/D-14.npy  \n","  inflating: data/data/train/D-15.npy  \n","  inflating: data/data/train/D-16.npy  \n","  inflating: data/data/train/D-2.npy  \n","  inflating: data/data/train/D-3.npy  \n","  inflating: data/data/train/D-4.npy  \n","  inflating: data/data/train/D-5.npy  \n","  inflating: data/data/train/D-6.npy  \n","  inflating: data/data/train/D-7.npy  \n","  inflating: data/data/train/D-8.npy  \n","  inflating: data/data/train/D-9.npy  \n","  inflating: data/data/train/E-1.npy  \n","  inflating: data/data/train/E-10.npy  \n","  inflating: data/data/train/E-11.npy  \n","  inflating: data/data/train/E-12.npy  \n","  inflating: data/data/train/E-13.npy  \n","  inflating: data/data/train/E-2.npy  \n","  inflating: data/data/train/E-3.npy  \n","  inflating: data/data/train/E-4.npy  \n","  inflating: data/data/train/E-5.npy  \n","  inflating: data/data/train/E-6.npy  \n","  inflating: data/data/train/E-7.npy  \n","  inflating: data/data/train/E-8.npy  \n","  inflating: data/data/train/E-9.npy  \n","  inflating: data/data/train/F-1.npy  \n","  inflating: data/data/train/F-2.npy  \n","  inflating: data/data/train/F-3.npy  \n","  inflating: data/data/train/F-4.npy  \n","  inflating: data/data/train/F-5.npy  \n","  inflating: data/data/train/F-7.npy  \n","  inflating: data/data/train/F-8.npy  \n","  inflating: data/data/train/G-1.npy  \n","  inflating: data/data/train/G-2.npy  \n","  inflating: data/data/train/G-3.npy  \n","  inflating: data/data/train/G-4.npy  \n","  inflating: data/data/train/G-6.npy  \n","  inflating: data/data/train/G-7.npy  \n","  inflating: data/data/train/M-1.npy  \n","  inflating: data/data/train/M-2.npy  \n","  inflating: data/data/train/M-3.npy  \n","  inflating: data/data/train/M-4.npy  \n","  inflating: data/data/train/M-5.npy  \n","  inflating: data/data/train/M-6.npy  \n","  inflating: data/data/train/M-7.npy  \n","  inflating: data/data/train/P-1.npy  \n","  inflating: data/data/train/P-10.npy  \n","  inflating: data/data/train/P-11.npy  \n","  inflating: data/data/train/P-14.npy  \n","  inflating: data/data/train/P-15.npy  \n","  inflating: data/data/train/P-2.npy  \n","  inflating: data/data/train/P-3.npy  \n","  inflating: data/data/train/P-4.npy  \n","  inflating: data/data/train/P-7.npy  \n","  inflating: data/data/train/R-1.npy  \n","  inflating: data/data/train/S-1.npy  \n","  inflating: data/data/train/S-2.npy  \n","  inflating: data/data/train/T-1.npy  \n","  inflating: data/data/train/T-10.npy  \n","  inflating: data/data/train/T-12.npy  \n","  inflating: data/data/train/T-13.npy  \n","  inflating: data/data/train/T-2.npy  \n","  inflating: data/data/train/T-3.npy  \n","  inflating: data/data/train/T-4.npy  \n","  inflating: data/data/train/T-5.npy  \n","  inflating: data/data/train/T-8.npy  \n","  inflating: data/data/train/T-9.npy  \n","  inflating: labeled_anomalies.csv   \n"]}]},{"cell_type":"markdown","source":["## Setup the dataset"],"metadata":{"id":"CNYIVLT3FAuk"}},{"cell_type":"code","source":["DRIVE = \"drive/MyDrive/Colab Notebooks/ELTE/DSLAB/ServerMachineDataset/\"\n","DRIVE= \"ServerMachineDataset/\"\n","MACHINE = \"machine-1-1.txt\"\n","TRAIN_DATASET = DRIVE + \"train/\" + MACHINE\n","TEST_DATASET = DRIVE + \"test/\" + MACHINE\n","TEST_LABEL_DATASET = DRIVE + \"test_label/\" + MACHINE\n","\n","metric = pd.read_csv(TRAIN_DATASET, header=None)\n","metric_test = pd.read_csv(TEST_DATASET, header=None)\n","true_anomalies = pd.read_csv(TEST_LABEL_DATASET, header=None)[0].to_numpy()"],"metadata":{"id":"fn5fwngg0fW8","executionInfo":{"status":"ok","timestamp":1759671038594,"user_tz":-120,"elapsed":180,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["metric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"IEXgrPZMGUYR","executionInfo":{"status":"ok","timestamp":1759671039868,"user_tz":-120,"elapsed":17,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"52d01e8f-e7c4-45dd-d78f-3a766ecd5f67"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             0         1         2         3    4         5         6    7   \\\n","0      0.032258  0.039195  0.027871  0.024390  0.0  0.915385  0.343691  0.0   \n","1      0.043011  0.048729  0.033445  0.025552  0.0  0.915385  0.344633  0.0   \n","2      0.043011  0.034958  0.032330  0.025552  0.0  0.915385  0.344633  0.0   \n","3      0.032258  0.028602  0.030100  0.024390  0.0  0.912821  0.342750  0.0   \n","4      0.032258  0.019068  0.026756  0.023229  0.0  0.912821  0.342750  0.0   \n","...         ...       ...       ...       ...  ...       ...       ...  ...   \n","28474  0.075269  0.046610  0.071349  0.076655  0.0  0.928205  0.269303  0.0   \n","28475  0.086022  0.070975  0.075808  0.077816  0.0  0.930769  0.269303  0.0   \n","28476  0.086022  0.065678  0.073579  0.076655  0.0  0.935897  0.270245  0.0   \n","28477  0.086022  0.056144  0.068004  0.074332  0.0  0.933333  0.271186  0.0   \n","28478  0.075269  0.081568  0.072464  0.075494  0.0  0.933333  0.273070  0.0   \n","\n","             8         9   ...   28        29        30        31        32  \\\n","0      0.020011  0.000122  ...  0.0  0.004298  0.029993  0.022131  0.000000   \n","1      0.019160  0.001722  ...  0.0  0.004298  0.030041  0.028821  0.000000   \n","2      0.020011  0.000122  ...  0.0  0.004298  0.026248  0.021101  0.000000   \n","3      0.021289  0.000000  ...  0.0  0.004298  0.030169  0.025733  0.000000   \n","4      0.018734  0.000000  ...  0.0  0.004298  0.027240  0.022645  0.000000   \n","...         ...       ...  ...  ...       ...       ...       ...       ...   \n","28474  0.031649  0.000244  ...  0.0  0.008596  0.068980  0.049408  0.000386   \n","28475  0.029946  0.000244  ...  0.0  0.008596  0.073029  0.055584  0.000386   \n","28476  0.030372  0.000244  ...  0.0  0.008596  0.070516  0.048893  0.000386   \n","28477  0.032643  0.000244  ...  0.0  0.008596  0.070308  0.055069  0.000386   \n","28478  0.031791  0.000000  ...  0.0  0.008596  0.067924  0.048893  0.000386   \n","\n","             33        34        35   36   37  \n","0      0.000045  0.034677  0.034747  0.0  0.0  \n","1      0.000045  0.035763  0.035833  0.0  0.0  \n","2      0.000045  0.033012  0.033082  0.0  0.0  \n","3      0.000022  0.035112  0.035182  0.0  0.0  \n","4      0.000034  0.033447  0.033517  0.0  0.0  \n","...         ...       ...       ...  ...  ...  \n","28474  0.000034  0.064504  0.064572  0.0  0.0  \n","28475  0.000034  0.067690  0.067757  0.0  0.0  \n","28476  0.000034  0.064866  0.064934  0.0  0.0  \n","28477  0.000045  0.067111  0.067178  0.0  0.0  \n","28478  0.000022  0.065011  0.065079  0.0  0.0  \n","\n","[28479 rows x 38 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.032258</td>\n","      <td>0.039195</td>\n","      <td>0.027871</td>\n","      <td>0.024390</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.343691</td>\n","      <td>0.0</td>\n","      <td>0.020011</td>\n","      <td>0.000122</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.029993</td>\n","      <td>0.022131</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.034677</td>\n","      <td>0.034747</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.043011</td>\n","      <td>0.048729</td>\n","      <td>0.033445</td>\n","      <td>0.025552</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.344633</td>\n","      <td>0.0</td>\n","      <td>0.019160</td>\n","      <td>0.001722</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.030041</td>\n","      <td>0.028821</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.035763</td>\n","      <td>0.035833</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.043011</td>\n","      <td>0.034958</td>\n","      <td>0.032330</td>\n","      <td>0.025552</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.344633</td>\n","      <td>0.0</td>\n","      <td>0.020011</td>\n","      <td>0.000122</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.026248</td>\n","      <td>0.021101</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.033012</td>\n","      <td>0.033082</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.032258</td>\n","      <td>0.028602</td>\n","      <td>0.030100</td>\n","      <td>0.024390</td>\n","      <td>0.0</td>\n","      <td>0.912821</td>\n","      <td>0.342750</td>\n","      <td>0.0</td>\n","      <td>0.021289</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.030169</td>\n","      <td>0.025733</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.035112</td>\n","      <td>0.035182</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.032258</td>\n","      <td>0.019068</td>\n","      <td>0.026756</td>\n","      <td>0.023229</td>\n","      <td>0.0</td>\n","      <td>0.912821</td>\n","      <td>0.342750</td>\n","      <td>0.0</td>\n","      <td>0.018734</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.027240</td>\n","      <td>0.022645</td>\n","      <td>0.000000</td>\n","      <td>0.000034</td>\n","      <td>0.033447</td>\n","      <td>0.033517</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>28474</th>\n","      <td>0.075269</td>\n","      <td>0.046610</td>\n","      <td>0.071349</td>\n","      <td>0.076655</td>\n","      <td>0.0</td>\n","      <td>0.928205</td>\n","      <td>0.269303</td>\n","      <td>0.0</td>\n","      <td>0.031649</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.068980</td>\n","      <td>0.049408</td>\n","      <td>0.000386</td>\n","      <td>0.000034</td>\n","      <td>0.064504</td>\n","      <td>0.064572</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28475</th>\n","      <td>0.086022</td>\n","      <td>0.070975</td>\n","      <td>0.075808</td>\n","      <td>0.077816</td>\n","      <td>0.0</td>\n","      <td>0.930769</td>\n","      <td>0.269303</td>\n","      <td>0.0</td>\n","      <td>0.029946</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.073029</td>\n","      <td>0.055584</td>\n","      <td>0.000386</td>\n","      <td>0.000034</td>\n","      <td>0.067690</td>\n","      <td>0.067757</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28476</th>\n","      <td>0.086022</td>\n","      <td>0.065678</td>\n","      <td>0.073579</td>\n","      <td>0.076655</td>\n","      <td>0.0</td>\n","      <td>0.935897</td>\n","      <td>0.270245</td>\n","      <td>0.0</td>\n","      <td>0.030372</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.070516</td>\n","      <td>0.048893</td>\n","      <td>0.000386</td>\n","      <td>0.000034</td>\n","      <td>0.064866</td>\n","      <td>0.064934</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28477</th>\n","      <td>0.086022</td>\n","      <td>0.056144</td>\n","      <td>0.068004</td>\n","      <td>0.074332</td>\n","      <td>0.0</td>\n","      <td>0.933333</td>\n","      <td>0.271186</td>\n","      <td>0.0</td>\n","      <td>0.032643</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.070308</td>\n","      <td>0.055069</td>\n","      <td>0.000386</td>\n","      <td>0.000045</td>\n","      <td>0.067111</td>\n","      <td>0.067178</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28478</th>\n","      <td>0.075269</td>\n","      <td>0.081568</td>\n","      <td>0.072464</td>\n","      <td>0.075494</td>\n","      <td>0.0</td>\n","      <td>0.933333</td>\n","      <td>0.273070</td>\n","      <td>0.0</td>\n","      <td>0.031791</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.067924</td>\n","      <td>0.048893</td>\n","      <td>0.000386</td>\n","      <td>0.000022</td>\n","      <td>0.065011</td>\n","      <td>0.065079</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28479 rows × 38 columns</p>\n","</div>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Preprocess the Dataset"],"metadata":{"id":"1El1ROXf7unv"}},{"cell_type":"code","source":["# Scale the values of the input metrics\n","scaler = MinMaxScaler()\n","metric_scaled = scaler.fit_transform(metric)\n","metric_tensor = torch.tensor(metric_scaled, dtype=torch.float32)\n","metric_scaled = pd.DataFrame(metric_scaled, index=metric.index, columns=metric.columns)\n","metric_scaled"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"7NKbSU7O7xZ9","executionInfo":{"status":"ok","timestamp":1758971433355,"user_tz":-120,"elapsed":112,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"a3f76ffd-f17d-42c1-cc9b-2cb67ad55207","collapsed":true},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             0         1         2         3    4         5         6    7   \\\n","0      0.065217  0.092965  0.100001  0.097221  0.0  0.915385  0.953003  0.0   \n","1      0.086957  0.115578  0.120001  0.101853  0.0  0.915385  0.955615  0.0   \n","2      0.086957  0.082915  0.116000  0.101853  0.0  0.915385  0.955615  0.0   \n","3      0.065217  0.067840  0.107999  0.097221  0.0  0.912821  0.950394  0.0   \n","4      0.065217  0.045227  0.096000  0.092593  0.0  0.912821  0.950394  0.0   \n","...         ...       ...       ...       ...  ...       ...       ...  ...   \n","28474  0.152174  0.110552  0.256000  0.305555  0.0  0.928205  0.746736  0.0   \n","28475  0.173914  0.168343  0.271999  0.310183  0.0  0.930769  0.746736  0.0   \n","28476  0.173914  0.155779  0.264001  0.305555  0.0  0.935897  0.749348  0.0   \n","28477  0.173914  0.133166  0.243998  0.296296  0.0  0.933333  0.751958  0.0   \n","28478  0.152174  0.193468  0.260001  0.300928  0.0  0.933333  0.757182  0.0   \n","\n","             8         9   ...   28        29        30        31        32  \\\n","0      0.169468  0.000869  ...  0.0  0.150002  0.088565  0.052083  0.000000   \n","1      0.162261  0.012262  ...  0.0  0.150002  0.088743  0.074651  0.000000   \n","2      0.169468  0.000869  ...  0.0  0.150002  0.074656  0.048609  0.000000   \n","3      0.180291  0.000000  ...  0.0  0.150002  0.089218  0.064234  0.000000   \n","4      0.158654  0.000000  ...  0.0  0.150002  0.078340  0.053817  0.000000   \n","...         ...       ...  ...  ...       ...       ...       ...       ...   \n","28474  0.268028  0.001737  ...  0.0  0.300003  0.233357  0.144096  0.166523   \n","28475  0.253606  0.001737  ...  0.0  0.300003  0.248395  0.164929  0.166523   \n","28476  0.257213  0.001737  ...  0.0  0.300003  0.239062  0.142359  0.166523   \n","28477  0.276446  0.001737  ...  0.0  0.300003  0.238289  0.163192  0.166523   \n","28478  0.269230  0.000000  ...  0.0  0.300003  0.229435  0.142359  0.166523   \n","\n","             33        34        35   36   37  \n","0      0.035405  0.085685  0.085926  0.0  0.0  \n","1      0.035405  0.089275  0.089517  0.0  0.0  \n","2      0.035405  0.080180  0.080421  0.0  0.0  \n","3      0.017309  0.087123  0.087364  0.0  0.0  \n","4      0.026751  0.081618  0.081859  0.0  0.0  \n","...         ...       ...       ...  ...  ...  \n","28474  0.026751  0.184297  0.184538  0.0  0.0  \n","28475  0.026751  0.194830  0.195069  0.0  0.0  \n","28476  0.026751  0.185493  0.185735  0.0  0.0  \n","28477  0.035405  0.192916  0.193155  0.0  0.0  \n","28478  0.017309  0.185973  0.186214  0.0  0.0  \n","\n","[28479 rows x 38 columns]"],"text/html":["\n","  <div id=\"df-c48209ce-c966-47a7-ac16-49af18a079b8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.065217</td>\n","      <td>0.092965</td>\n","      <td>0.100001</td>\n","      <td>0.097221</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.953003</td>\n","      <td>0.0</td>\n","      <td>0.169468</td>\n","      <td>0.000869</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.150002</td>\n","      <td>0.088565</td>\n","      <td>0.052083</td>\n","      <td>0.000000</td>\n","      <td>0.035405</td>\n","      <td>0.085685</td>\n","      <td>0.085926</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.086957</td>\n","      <td>0.115578</td>\n","      <td>0.120001</td>\n","      <td>0.101853</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.955615</td>\n","      <td>0.0</td>\n","      <td>0.162261</td>\n","      <td>0.012262</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.150002</td>\n","      <td>0.088743</td>\n","      <td>0.074651</td>\n","      <td>0.000000</td>\n","      <td>0.035405</td>\n","      <td>0.089275</td>\n","      <td>0.089517</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.086957</td>\n","      <td>0.082915</td>\n","      <td>0.116000</td>\n","      <td>0.101853</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.955615</td>\n","      <td>0.0</td>\n","      <td>0.169468</td>\n","      <td>0.000869</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.150002</td>\n","      <td>0.074656</td>\n","      <td>0.048609</td>\n","      <td>0.000000</td>\n","      <td>0.035405</td>\n","      <td>0.080180</td>\n","      <td>0.080421</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.065217</td>\n","      <td>0.067840</td>\n","      <td>0.107999</td>\n","      <td>0.097221</td>\n","      <td>0.0</td>\n","      <td>0.912821</td>\n","      <td>0.950394</td>\n","      <td>0.0</td>\n","      <td>0.180291</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.150002</td>\n","      <td>0.089218</td>\n","      <td>0.064234</td>\n","      <td>0.000000</td>\n","      <td>0.017309</td>\n","      <td>0.087123</td>\n","      <td>0.087364</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.065217</td>\n","      <td>0.045227</td>\n","      <td>0.096000</td>\n","      <td>0.092593</td>\n","      <td>0.0</td>\n","      <td>0.912821</td>\n","      <td>0.950394</td>\n","      <td>0.0</td>\n","      <td>0.158654</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.150002</td>\n","      <td>0.078340</td>\n","      <td>0.053817</td>\n","      <td>0.000000</td>\n","      <td>0.026751</td>\n","      <td>0.081618</td>\n","      <td>0.081859</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>28474</th>\n","      <td>0.152174</td>\n","      <td>0.110552</td>\n","      <td>0.256000</td>\n","      <td>0.305555</td>\n","      <td>0.0</td>\n","      <td>0.928205</td>\n","      <td>0.746736</td>\n","      <td>0.0</td>\n","      <td>0.268028</td>\n","      <td>0.001737</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.300003</td>\n","      <td>0.233357</td>\n","      <td>0.144096</td>\n","      <td>0.166523</td>\n","      <td>0.026751</td>\n","      <td>0.184297</td>\n","      <td>0.184538</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28475</th>\n","      <td>0.173914</td>\n","      <td>0.168343</td>\n","      <td>0.271999</td>\n","      <td>0.310183</td>\n","      <td>0.0</td>\n","      <td>0.930769</td>\n","      <td>0.746736</td>\n","      <td>0.0</td>\n","      <td>0.253606</td>\n","      <td>0.001737</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.300003</td>\n","      <td>0.248395</td>\n","      <td>0.164929</td>\n","      <td>0.166523</td>\n","      <td>0.026751</td>\n","      <td>0.194830</td>\n","      <td>0.195069</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28476</th>\n","      <td>0.173914</td>\n","      <td>0.155779</td>\n","      <td>0.264001</td>\n","      <td>0.305555</td>\n","      <td>0.0</td>\n","      <td>0.935897</td>\n","      <td>0.749348</td>\n","      <td>0.0</td>\n","      <td>0.257213</td>\n","      <td>0.001737</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.300003</td>\n","      <td>0.239062</td>\n","      <td>0.142359</td>\n","      <td>0.166523</td>\n","      <td>0.026751</td>\n","      <td>0.185493</td>\n","      <td>0.185735</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28477</th>\n","      <td>0.173914</td>\n","      <td>0.133166</td>\n","      <td>0.243998</td>\n","      <td>0.296296</td>\n","      <td>0.0</td>\n","      <td>0.933333</td>\n","      <td>0.751958</td>\n","      <td>0.0</td>\n","      <td>0.276446</td>\n","      <td>0.001737</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.300003</td>\n","      <td>0.238289</td>\n","      <td>0.163192</td>\n","      <td>0.166523</td>\n","      <td>0.035405</td>\n","      <td>0.192916</td>\n","      <td>0.193155</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28478</th>\n","      <td>0.152174</td>\n","      <td>0.193468</td>\n","      <td>0.260001</td>\n","      <td>0.300928</td>\n","      <td>0.0</td>\n","      <td>0.933333</td>\n","      <td>0.757182</td>\n","      <td>0.0</td>\n","      <td>0.269230</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.300003</td>\n","      <td>0.229435</td>\n","      <td>0.142359</td>\n","      <td>0.166523</td>\n","      <td>0.017309</td>\n","      <td>0.185973</td>\n","      <td>0.186214</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28479 rows × 38 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c48209ce-c966-47a7-ac16-49af18a079b8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c48209ce-c966-47a7-ac16-49af18a079b8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c48209ce-c966-47a7-ac16-49af18a079b8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-c6436a3e-f579-46e4-9a6a-a8fcddad9521\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6436a3e-f579-46e4-9a6a-a8fcddad9521')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-c6436a3e-f579-46e4-9a6a-a8fcddad9521 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_34f6c22a-6d9e-4fd5-b1cf-8d34451ae3b7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metric_scaled')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_34f6c22a-6d9e-4fd5-b1cf-8d34451ae3b7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('metric_scaled');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"metric_scaled"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Scaled"],"metadata":{"id":"QCGl5bXwDbqG"}},{"cell_type":"code","source":["# create train and test dataloaders\n","metric.interpolate(inplace=True)\n","metric.bfill(inplace=True)\n","metric_tensor = metric.values\n","\n","metric_test.interpolate(inplace=True)\n","metric_test.bfill(inplace=True)\n","metric_test_tensor = metric_test.values\n","\n","sequence_length = 30\n","sequences = []\n","for i in range(metric_tensor.shape[0] - sequence_length + 1):\n","  sequences.append(metric_tensor[i:i + sequence_length])\n","\n","\n","train_data, val_data = train_test_split(sequences, test_size=0.3, random_state=42) # 70% train, 30% temp\n","\n","test_sequences = []\n","for i in range(metric_test_tensor.shape[0] - sequence_length + 1):\n","  test_sequences.append(metric_test_tensor[i:i + sequence_length])\n","\n","\n","batch_size = 32\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(dataset=test_sequences, batch_size=batch_size, shuffle=False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"wwGabsp3zXBp","executionInfo":{"status":"ok","timestamp":1759671047657,"user_tz":-120,"elapsed":26,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["sequences[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRct-vXp0kgm","executionInfo":{"status":"ok","timestamp":1759671050008,"user_tz":-120,"elapsed":4,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"5f5dcda5-ee47-407f-8c2a-f6a622d6a714"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30, 38)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Define the Network"],"metadata":{"id":"J_nZLXQN7yKW"}},{"cell_type":"markdown","source":["## LSTM"],"metadata":{"id":"10IVAB_oBek_"}},{"cell_type":"code","source":["class LSTMEncoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n","        super(LSTMEncoder, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n","        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x):\n","        _, (h_n, _) = self.lstm(x)  # h_n: (num_layers, batch, hidden_dim)\n","        h = h_n[-1]  # take the output of the last layer\n","        return self.fc_mean(h), self.fc_logvar(h)\n","\n","\n","class LSTMDecoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim, sequence_length, num_layers=1):\n","        super(LSTMDecoder, self).__init__()\n","        self.sequence_length = sequence_length\n","        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.output_layer = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, z):\n","        # Repeat z for each timestep\n","        hidden = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)\n","        out, _ = self.lstm(hidden)\n","        return self.output_layer(out)\n","\n","\n","class LSTMVAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, sequence_length, num_layers=1, device='cpu'):\n","        super(LSTMVAE, self).__init__()\n","        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim, num_layers).to(device)\n","        self.decoder = LSTMDecoder(latent_dim, hidden_dim, input_dim, sequence_length, num_layers).to(device)\n","\n","    def reparameterize(self, mean, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mean + eps * std\n","\n","    def forward(self, x):\n","        mean, logvar = self.encoder(x)\n","        z = self.reparameterize(mean, logvar)\n","        x_recon = self.decoder(z)\n","        return x_recon, mean, logvar"],"metadata":{"id":"DVML-8z6BZRt","executionInfo":{"status":"ok","timestamp":1759671052192,"user_tz":-120,"elapsed":1,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["input_dim = 38\n","hidden_dim = 128\n","latent_dim = 32\n","num_layers = 1\n","\n","model = LSTMVAE(input_dim=input_dim,\n","                hidden_dim=hidden_dim,\n","                latent_dim=latent_dim,\n","                sequence_length=sequence_length,\n","                num_layers=num_layers,\n","                device=device).to(device)\n","optimizer = Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"_-HozbDBQjp_","executionInfo":{"status":"ok","timestamp":1759671055100,"user_tz":-120,"elapsed":1003,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Support functions"],"metadata":{"id":"cCvCWPSaBjTr"}},{"cell_type":"code","source":["def loss_function(x, x_hat, mean, log_var):\n","    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n","    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n","\n","    return reproduction_loss + KLD"],"metadata":{"id":"oC9p-xxjMZa4","executionInfo":{"status":"ok","timestamp":1759671055105,"user_tz":-120,"elapsed":3,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def save_model(model):\n","    model_state = {\n","        'input_dim':input_dim,\n","        'latent_dim':latent_dim,\n","        'hidden_dim':hidden_dim,\n","        'state_dict':model.state_dict()\n","    }\n","    torch.save(model_state,'vae.pth')"],"metadata":{"id":"jtWTMS6O0BO0","executionInfo":{"status":"ok","timestamp":1759671055755,"user_tz":-120,"elapsed":2,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"ftEj1OZs78Pk"}},{"cell_type":"markdown","source":["## LSTM"],"metadata":{"id":"vG5kpDZaQxEw"}},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","\n","scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n","\n","# SPO optimizer - optuna\n","# bayesian hyperparameter tuning\n","# grid search - slow for DL\n","\n","def train_model(model, train_loader, val_loader, optimizer, loss_fn, scheduler, num_epochs=10, device='cpu'):\n","    torch.cuda.empty_cache()\n","    train_losses = []\n","    val_losses = []\n","\n","    early_stop_tolerant_count = 0\n","    early_stop_tolerant = 10\n","    best_loss = float('inf')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    for epoch in range(num_epochs):\n","        train_loss = 0.0\n","        model.train()\n","        for batch in train_loader:\n","            batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","\n","            optimizer.zero_grad()\n","\n","            recon_batch, mean, logvar = model(batch)\n","            loss = loss_fn(recon_batch, batch, mean, logvar)\n","\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_loss /= len(train_loader)\n","        train_losses.append(train_loss)\n","\n","        # Validation\n","        model.eval()\n","        valid_loss = 0.0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","                recon_batch, mean, logvar = model(batch)\n","                loss = loss_fn(recon_batch, batch, mean, logvar)\n","                valid_loss += loss.item()\n","\n","        valid_loss /= len(val_loader)\n","        val_losses.append(valid_loss)\n","\n","        scheduler.step(valid_loss)\n","\n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            early_stop_tolerant_count = 0\n","        else:\n","            early_stop_tolerant_count += 1\n","\n","        print(f\"Epoch {epoch+1:04d}: train loss {train_loss:.4f}, valid loss {valid_loss:.4f}\")\n","\n","        if early_stop_tolerant_count >= early_stop_tolerant:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    model.load_state_dict(best_model_wts)\n","    print(\"Finished Training.\")\n","    return train_losses, val_losses\n","\n","#train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, loss_function, scheduler, num_epochs=100, device=device)"],"metadata":{"collapsed":true,"id":"uCnBGx0dQxLi","executionInfo":{"status":"ok","timestamp":1759671060542,"user_tz":-120,"elapsed":2,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cda315a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759683598243,"user_tz":-120,"elapsed":12532491,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"d160cc59-2eaa-4b2a-fa4d-6f552cb1158b"},"source":["import optuna\n","\n","def objective(trial):\n","    # Define hyperparameters to tune\n","    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n","    latent_dim = trial.suggest_categorical(\"latent_dim\", [16, 32, 64])\n","    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n","\n","    # Create DataLoader with the suggested batch size\n","    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n","\n","    # Initialize model, optimizer, and scheduler with suggested hyperparameters\n","    model = LSTMVAE(input_dim=input_dim,\n","                    hidden_dim=hidden_dim,\n","                    latent_dim=latent_dim,\n","                    sequence_length=sequence_length,\n","                    num_layers=num_layers,\n","                    device=device).to(device)\n","    optimizer = Adam(model.parameters(), lr=learning_rate)\n","    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n","\n","    # Train the model (using a smaller number of epochs for tuning)\n","    train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, loss_function, scheduler, num_epochs=20, device=device)\n","\n","    # Return the validation loss for Optuna to minimize\n","    return val_losses[-1]\n","\n","# Run the Optuna study\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=50)\n","\n","# Print the best hyperparameters\n","print(\"Best hyperparameters: \", study.best_params)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\beren\\Desktop\\notebooks\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[I 2025-10-05 15:31:05,829] A new study created in memory with name: no-name-3c4fa056-ed4e-426a-90d0-a26aadda8bfb\n","C:\\Users\\beren\\AppData\\Local\\Temp\\ipykernel_5972\\428586503.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n","C:\\Users\\beren\\AppData\\Local\\Temp\\ipykernel_5972\\1313062830.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","C:\\Users\\beren\\AppData\\Local\\Temp\\ipykernel_5972\\1313062830.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0001: train loss 476.5092, valid loss 188.7910\n","Epoch 0002: train loss 156.3893, valid loss 131.1115\n","Epoch 0003: train loss 115.2635, valid loss 104.6491\n","Epoch 0004: train loss 96.1119, valid loss 88.6886\n","Epoch 0005: train loss 81.3105, valid loss 75.0022\n","Epoch 0006: train loss 71.6718, valid loss 69.9957\n","Epoch 0007: train loss 67.3188, valid loss 66.4062\n","Epoch 0008: train loss 65.1471, valid loss 65.8117\n","Epoch 0009: train loss 63.7338, valid loss 64.1002\n","Epoch 0010: train loss 63.1082, valid loss 63.0475\n","Epoch 0011: train loss 62.8186, valid loss 64.2839\n","Epoch 0012: train loss 62.5271, valid loss 62.6748\n","Epoch 0013: train loss 62.5977, valid loss 63.1927\n","Epoch 0014: train loss 62.2373, valid loss 62.4420\n","Epoch 0015: train loss 62.3760, valid loss 63.2046\n","Epoch 0016: train loss 62.2536, valid loss 62.5954\n","Epoch 0017: train loss 62.0696, valid loss 62.6462\n","Epoch 0018: train loss 62.1868, valid loss 62.7016\n","Epoch 0019: train loss 61.9733, valid loss 62.0017\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 15:39:29,346] Trial 0 finished with value: 62.167745911673215 and parameters: {'hidden_dim': 256, 'latent_dim': 16, 'num_layers': 2, 'learning_rate': 1.532267139387774e-05, 'batch_size': 32}. Best is trial 0 with value: 62.167745911673215.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 61.6651, valid loss 62.1677\n","Finished Training.\n","Epoch 0001: train loss 280.0942, valid loss 119.3896\n","Epoch 0002: train loss 98.5717, valid loss 82.6260\n","Epoch 0003: train loss 71.5608, valid loss 61.3643\n","Epoch 0004: train loss 55.1556, valid loss 51.1007\n","Epoch 0005: train loss 47.8795, valid loss 45.8650\n","Epoch 0006: train loss 44.3337, valid loss 43.1686\n","Epoch 0007: train loss 42.0228, valid loss 41.3335\n","Epoch 0008: train loss 40.7326, valid loss 40.7865\n","Epoch 0009: train loss 40.0880, valid loss 40.0295\n","Epoch 0010: train loss 40.0164, valid loss 39.3296\n","Epoch 0011: train loss 39.5045, valid loss 39.8813\n","Epoch 0012: train loss 39.2744, valid loss 39.2020\n","Epoch 0013: train loss 38.7457, valid loss 39.0126\n","Epoch 0014: train loss 38.4437, valid loss 38.6754\n","Epoch 0015: train loss 37.6230, valid loss 37.5876\n","Epoch 0016: train loss 36.6945, valid loss 35.8357\n","Epoch 0017: train loss 34.3270, valid loss 33.5787\n","Epoch 0018: train loss 32.7599, valid loss 32.5671\n","Epoch 0019: train loss 32.0964, valid loss 32.2265\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 15:43:51,430] Trial 1 finished with value: 31.887346971347537 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'num_layers': 2, 'learning_rate': 1.633171211752766e-05, 'batch_size': 16}. Best is trial 1 with value: 31.887346971347537.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 31.7285, valid loss 31.8873\n","Finished Training.\n","Epoch 0001: train loss 1201.2604, valid loss 533.4191\n","Epoch 0002: train loss 318.3058, valid loss 267.9529\n","Epoch 0003: train loss 242.9984, valid loss 225.1132\n","Epoch 0004: train loss 208.7097, valid loss 194.4630\n","Epoch 0005: train loss 182.3296, valid loss 171.2436\n","Epoch 0006: train loss 161.8117, valid loss 153.7719\n","Epoch 0007: train loss 144.0517, valid loss 135.9222\n","Epoch 0008: train loss 126.7903, valid loss 117.5821\n","Epoch 0009: train loss 112.4499, valid loss 107.7926\n","Epoch 0010: train loss 102.9794, valid loss 100.7696\n","Epoch 0011: train loss 97.8520, valid loss 95.1000\n","Epoch 0012: train loss 93.9515, valid loss 91.8636\n","Epoch 0013: train loss 90.1615, valid loss 90.9016\n","Epoch 0014: train loss 87.4808, valid loss 86.6405\n","Epoch 0015: train loss 85.4627, valid loss 86.0128\n","Epoch 0016: train loss 84.1065, valid loss 84.1482\n","Epoch 0017: train loss 82.7004, valid loss 82.8398\n","Epoch 0018: train loss 82.2206, valid loss 81.7602\n","Epoch 0019: train loss 80.9540, valid loss 81.1002\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 15:45:44,086] Trial 2 finished with value: 81.06440326247768 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 2, 'learning_rate': 1.2917416479089541e-05, 'batch_size': 32}. Best is trial 1 with value: 31.887346971347537.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 80.2840, valid loss 81.0644\n","Finished Training.\n","Epoch 0001: train loss 854.9954, valid loss 355.2513\n","Epoch 0002: train loss 310.2262, valid loss 276.7531\n","Epoch 0003: train loss 236.0464, valid loss 184.9817\n","Epoch 0004: train loss 169.1864, valid loss 148.5584\n","Epoch 0005: train loss 136.8003, valid loss 132.0108\n","Epoch 0006: train loss 129.5994, valid loss 129.6242\n","Epoch 0007: train loss 128.0918, valid loss 128.1365\n","Epoch 0008: train loss 127.0589, valid loss 126.0242\n","Epoch 0009: train loss 125.4277, valid loss 125.9707\n","Epoch 0010: train loss 125.0067, valid loss 124.5280\n","Epoch 0011: train loss 124.6053, valid loss 125.7305\n","Epoch 0012: train loss 124.3453, valid loss 125.7751\n","Epoch 0013: train loss 124.0148, valid loss 125.1696\n","Epoch 0014: train loss 123.2610, valid loss 125.1563\n","Epoch 0015: train loss 123.4201, valid loss 124.2030\n","Epoch 0016: train loss 123.7151, valid loss 124.9707\n","Epoch 0017: train loss 122.9338, valid loss 124.3584\n","Epoch 0018: train loss 123.3537, valid loss 123.2335\n","Epoch 0019: train loss 122.8520, valid loss 123.4863\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 15:57:52,268] Trial 3 finished with value: 123.02273149632695 and parameters: {'hidden_dim': 256, 'latent_dim': 16, 'num_layers': 3, 'learning_rate': 4.3303111968080846e-05, 'batch_size': 64}. Best is trial 1 with value: 31.887346971347537.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 122.3926, valid loss 123.0227\n","Finished Training.\n","Epoch 0001: train loss 206.7976, valid loss 94.7093\n","Epoch 0002: train loss 85.4394, valid loss 80.8074\n","Epoch 0003: train loss 79.7684, valid loss 79.7375\n","Epoch 0004: train loss 78.2985, valid loss 74.5841\n","Epoch 0005: train loss 66.1053, valid loss 65.8133\n","Epoch 0006: train loss 63.0063, valid loss 63.4854\n","Epoch 0007: train loss 62.8384, valid loss 62.8582\n","Epoch 0008: train loss 62.2627, valid loss 64.3432\n","Epoch 0009: train loss 62.4472, valid loss 63.1312\n","Epoch 0010: train loss 62.0214, valid loss 62.9465\n","Epoch 0011: train loss 62.1338, valid loss 61.9065\n","Epoch 0012: train loss 61.8323, valid loss 62.5115\n","Epoch 0013: train loss 61.8402, valid loss 62.6206\n","Epoch 0014: train loss 61.6410, valid loss 62.5137\n","Epoch 0015: train loss 61.9009, valid loss 62.7608\n","Epoch 0016: train loss 61.5416, valid loss 62.4553\n","Epoch 0017: train loss 61.8905, valid loss 61.7862\n","Epoch 0018: train loss 61.6656, valid loss 62.2636\n","Epoch 0019: train loss 61.5809, valid loss 62.3334\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:01:06,909] Trial 4 finished with value: 62.47479042310393 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'num_layers': 2, 'learning_rate': 0.0001862790056063284, 'batch_size': 32}. Best is trial 1 with value: 31.887346971347537.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 61.6129, valid loss 62.4748\n","Finished Training.\n","Epoch 0001: train loss 268.5861, valid loss 105.2415\n","Epoch 0002: train loss 86.4179, valid loss 70.1406\n","Epoch 0003: train loss 61.8442, valid loss 56.1289\n","Epoch 0004: train loss 52.3493, valid loss 49.1803\n","Epoch 0005: train loss 46.1191, valid loss 45.0072\n","Epoch 0006: train loss 43.2704, valid loss 42.7782\n","Epoch 0007: train loss 41.1619, valid loss 40.9419\n","Epoch 0008: train loss 40.3174, valid loss 40.5982\n","Epoch 0009: train loss 39.7896, valid loss 40.2071\n","Epoch 0010: train loss 39.2846, valid loss 39.3987\n","Epoch 0011: train loss 38.9638, valid loss 39.0824\n","Epoch 0012: train loss 38.4048, valid loss 38.7675\n","Epoch 0013: train loss 37.9135, valid loss 37.0524\n","Epoch 0014: train loss 36.0384, valid loss 36.6233\n","Epoch 0015: train loss 33.4143, valid loss 32.7996\n","Epoch 0016: train loss 32.0756, valid loss 32.5094\n","Epoch 0017: train loss 31.8976, valid loss 32.3059\n","Epoch 0018: train loss 31.8146, valid loss 31.8340\n","Epoch 0019: train loss 31.7020, valid loss 31.6321\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:07:22,675] Trial 5 finished with value: 32.3680366291089 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'num_layers': 3, 'learning_rate': 1.5339459573164413e-05, 'batch_size': 16}. Best is trial 1 with value: 31.887346971347537.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 31.7563, valid loss 32.3680\n","Finished Training.\n","Epoch 0001: train loss 95.6818, valid loss 45.1969\n","Epoch 0002: train loss 42.3364, valid loss 42.8967\n","Epoch 0003: train loss 40.4460, valid loss 41.0785\n","Epoch 0004: train loss 34.8020, valid loss 33.0683\n","Epoch 0005: train loss 32.3143, valid loss 33.0208\n","Epoch 0006: train loss 31.8122, valid loss 32.0715\n","Epoch 0007: train loss 31.5535, valid loss 31.5692\n","Epoch 0008: train loss 31.6057, valid loss 31.4205\n","Epoch 0009: train loss 31.4787, valid loss 31.7127\n","Epoch 0010: train loss 31.2371, valid loss 31.5526\n","Epoch 0011: train loss 31.3350, valid loss 31.3934\n","Epoch 0012: train loss 31.0643, valid loss 31.2008\n","Epoch 0013: train loss 31.2516, valid loss 31.3105\n","Epoch 0014: train loss 31.2376, valid loss 31.2875\n","Epoch 0015: train loss 30.9923, valid loss 31.3004\n","Epoch 0016: train loss 31.0178, valid loss 31.1445\n","Epoch 0017: train loss 31.0863, valid loss 31.1601\n","Epoch 0018: train loss 30.9812, valid loss 31.1173\n","Epoch 0019: train loss 31.0976, valid loss 31.4105\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:12:11,428] Trial 6 finished with value: 31.349870771058043 and parameters: {'hidden_dim': 256, 'latent_dim': 64, 'num_layers': 1, 'learning_rate': 0.00011876836603435518, 'batch_size': 16}. Best is trial 6 with value: 31.349870771058043.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8132, valid loss 31.3499\n","Finished Training.\n","Epoch 0001: train loss 151.0661, valid loss 69.1794\n","Epoch 0002: train loss 58.6543, valid loss 49.9583\n","Epoch 0003: train loss 44.7579, valid loss 42.3199\n","Epoch 0004: train loss 38.0355, valid loss 35.1899\n","Epoch 0005: train loss 34.0540, valid loss 33.5144\n","Epoch 0006: train loss 32.9231, valid loss 32.8670\n","Epoch 0007: train loss 32.3176, valid loss 32.2249\n","Epoch 0008: train loss 31.9612, valid loss 32.2703\n","Epoch 0009: train loss 31.7834, valid loss 31.7892\n","Epoch 0010: train loss 31.5685, valid loss 31.9115\n","Epoch 0011: train loss 31.3782, valid loss 31.5036\n","Epoch 0012: train loss 31.2829, valid loss 31.5536\n","Epoch 0013: train loss 31.2290, valid loss 31.4645\n","Epoch 0014: train loss 31.0799, valid loss 31.3525\n","Epoch 0015: train loss 30.9705, valid loss 31.5948\n","Epoch 0016: train loss 31.0654, valid loss 31.5107\n","Epoch 0017: train loss 30.9209, valid loss 31.3458\n","Epoch 0018: train loss 30.9892, valid loss 31.6591\n","Epoch 0019: train loss 31.0466, valid loss 31.4776\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:16:57,148] Trial 7 finished with value: 31.252281310406516 and parameters: {'hidden_dim': 256, 'latent_dim': 16, 'num_layers': 1, 'learning_rate': 1.858780821790057e-05, 'batch_size': 16}. Best is trial 7 with value: 31.252281310406516.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.7373, valid loss 31.2523\n","Finished Training.\n","Epoch 0001: train loss 92.9776, valid loss 42.9518\n","Epoch 0002: train loss 38.8383, valid loss 35.1010\n","Epoch 0003: train loss 32.7993, valid loss 33.1592\n","Epoch 0004: train loss 32.0108, valid loss 32.1885\n","Epoch 0005: train loss 31.7262, valid loss 31.5779\n","Epoch 0006: train loss 31.6376, valid loss 32.6613\n","Epoch 0007: train loss 31.5897, valid loss 32.0034\n","Epoch 0008: train loss 31.5433, valid loss 31.6467\n","Epoch 0009: train loss 31.3282, valid loss 31.3231\n","Epoch 0010: train loss 31.1483, valid loss 31.1888\n","Epoch 0011: train loss 31.0855, valid loss 31.5096\n","Epoch 0012: train loss 31.0331, valid loss 31.5121\n","Epoch 0013: train loss 31.0438, valid loss 31.3533\n","Epoch 0014: train loss 31.1193, valid loss 31.0371\n","Epoch 0015: train loss 31.1538, valid loss 31.4298\n","Epoch 0016: train loss 30.9751, valid loss 31.5018\n","Epoch 0017: train loss 31.0323, valid loss 31.5645\n","Epoch 0018: train loss 30.8733, valid loss 31.2716\n","Epoch 0019: train loss 30.9337, valid loss 31.1905\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:19:28,108] Trial 8 finished with value: 31.469819418946454 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'num_layers': 1, 'learning_rate': 0.00023613046047039402, 'batch_size': 16}. Best is trial 7 with value: 31.252281310406516.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.9073, valid loss 31.4698\n","Finished Training.\n","Epoch 0001: train loss 3789.5912, valid loss 3407.1953\n","Epoch 0002: train loss 3052.2098, valid loss 2643.0356\n","Epoch 0003: train loss 2045.4628, valid loss 1222.1254\n","Epoch 0004: train loss 869.0223, valid loss 707.5453\n","Epoch 0005: train loss 662.1183, valid loss 627.1348\n","Epoch 0006: train loss 598.3278, valid loss 568.0655\n","Epoch 0007: train loss 550.1613, valid loss 526.3577\n","Epoch 0008: train loss 510.8656, valid loss 490.3811\n","Epoch 0009: train loss 480.0672, valid loss 465.2064\n","Epoch 0010: train loss 450.7449, valid loss 438.1023\n","Epoch 0011: train loss 424.3816, valid loss 413.8419\n","Epoch 0012: train loss 401.8334, valid loss 390.5622\n","Epoch 0013: train loss 382.1460, valid loss 365.8374\n","Epoch 0014: train loss 359.6732, valid loss 353.8607\n","Epoch 0015: train loss 341.4972, valid loss 332.7974\n","Epoch 0016: train loss 321.7708, valid loss 315.8305\n","Epoch 0017: train loss 306.9150, valid loss 299.6052\n","Epoch 0018: train loss 293.3085, valid loss 284.1698\n","Epoch 0019: train loss 279.6017, valid loss 272.8625\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:20:13,174] Trial 9 finished with value: 264.3140296366677 and parameters: {'hidden_dim': 64, 'latent_dim': 16, 'num_layers': 1, 'learning_rate': 1.199355755332967e-05, 'batch_size': 64}. Best is trial 7 with value: 31.252281310406516.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 267.3901, valid loss 264.3140\n","Finished Training.\n","Epoch 0001: train loss 45.4054, valid loss 32.6330\n","Epoch 0002: train loss 32.7862, valid loss 32.2908\n","Epoch 0003: train loss 32.2743, valid loss 32.3453\n","Epoch 0004: train loss 32.0500, valid loss 32.6522\n","Epoch 0005: train loss 31.7759, valid loss 31.6530\n","Epoch 0006: train loss 31.5857, valid loss 31.9442\n","Epoch 0007: train loss 31.2053, valid loss 31.6520\n","Epoch 0008: train loss 31.3488, valid loss 31.4136\n","Epoch 0009: train loss 31.3424, valid loss 31.8933\n","Epoch 0010: train loss 31.0621, valid loss 31.8570\n","Epoch 0011: train loss 31.0199, valid loss 31.4737\n","Epoch 0012: train loss 31.0716, valid loss 31.1326\n","Epoch 0013: train loss 31.1228, valid loss 31.3819\n","Epoch 0014: train loss 31.0724, valid loss 31.1720\n","Epoch 0015: train loss 31.0627, valid loss 31.3350\n","Epoch 0016: train loss 30.9654, valid loss 30.9941\n","Epoch 0017: train loss 30.9396, valid loss 31.1104\n","Epoch 0018: train loss 30.8938, valid loss 31.1778\n","Epoch 0019: train loss 31.0352, valid loss 31.4017\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:25:00,962] Trial 10 finished with value: 31.31155541952183 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'num_layers': 1, 'learning_rate': 0.0006587569581187639, 'batch_size': 16}. Best is trial 7 with value: 31.252281310406516.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 31.0102, valid loss 31.3116\n","Finished Training.\n","Epoch 0001: train loss 45.4986, valid loss 33.6458\n","Epoch 0002: train loss 32.9922, valid loss 33.2486\n","Epoch 0003: train loss 32.4177, valid loss 32.3043\n","Epoch 0004: train loss 32.1298, valid loss 32.2001\n","Epoch 0005: train loss 31.7495, valid loss 32.5825\n","Epoch 0006: train loss 31.5296, valid loss 31.7830\n","Epoch 0007: train loss 31.5723, valid loss 32.0766\n","Epoch 0008: train loss 31.3268, valid loss 31.4853\n","Epoch 0009: train loss 31.2750, valid loss 31.6356\n","Epoch 0010: train loss 31.2582, valid loss 32.1082\n","Epoch 0011: train loss 31.1922, valid loss 31.2097\n","Epoch 0012: train loss 31.1328, valid loss 31.5465\n","Epoch 0013: train loss 31.0752, valid loss 31.8423\n","Epoch 0014: train loss 31.0188, valid loss 31.0642\n","Epoch 0015: train loss 30.9808, valid loss 31.6118\n","Epoch 0016: train loss 31.0430, valid loss 31.3138\n","Epoch 0017: train loss 30.8966, valid loss 31.2280\n","Epoch 0018: train loss 30.9732, valid loss 30.8873\n","Epoch 0019: train loss 30.8893, valid loss 31.4659\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:29:48,045] Trial 11 finished with value: 31.02073034365079 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'num_layers': 1, 'learning_rate': 0.0007528234362158605, 'batch_size': 16}. Best is trial 11 with value: 31.02073034365079.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.9925, valid loss 31.0207\n","Finished Training.\n","Epoch 0001: train loss 45.5559, valid loss 33.4448\n","Epoch 0002: train loss 32.9214, valid loss 33.0591\n","Epoch 0003: train loss 32.5360, valid loss 32.0651\n","Epoch 0004: train loss 31.8798, valid loss 32.4035\n","Epoch 0005: train loss 31.7890, valid loss 31.5585\n","Epoch 0006: train loss 31.4739, valid loss 32.1335\n","Epoch 0007: train loss 31.3557, valid loss 31.3562\n","Epoch 0008: train loss 31.3256, valid loss 31.4420\n","Epoch 0009: train loss 31.3936, valid loss 31.9655\n","Epoch 0010: train loss 31.2042, valid loss 31.5181\n","Epoch 0011: train loss 31.2574, valid loss 31.0356\n","Epoch 0012: train loss 31.0890, valid loss 31.4234\n","Epoch 0013: train loss 31.1245, valid loss 31.9092\n","Epoch 0014: train loss 31.0777, valid loss 31.7602\n","Epoch 0015: train loss 31.2917, valid loss 31.0171\n","Epoch 0016: train loss 31.0634, valid loss 31.1742\n","Epoch 0017: train loss 31.0552, valid loss 31.2769\n","Epoch 0018: train loss 31.1916, valid loss 31.0671\n","Epoch 0019: train loss 30.8935, valid loss 31.4086\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:34:36,109] Trial 12 finished with value: 31.741471822788654 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'num_layers': 1, 'learning_rate': 0.0009356449706514553, 'batch_size': 16}. Best is trial 11 with value: 31.02073034365079.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8381, valid loss 31.7415\n","Finished Training.\n","Epoch 0001: train loss 130.4286, valid loss 59.0390\n","Epoch 0002: train loss 49.1286, valid loss 44.8278\n","Epoch 0003: train loss 41.9192, valid loss 39.3494\n","Epoch 0004: train loss 37.3808, valid loss 34.7369\n","Epoch 0005: train loss 33.2778, valid loss 33.2947\n","Epoch 0006: train loss 32.5094, valid loss 32.6325\n","Epoch 0007: train loss 32.1784, valid loss 32.0884\n","Epoch 0008: train loss 31.9312, valid loss 32.1418\n","Epoch 0009: train loss 31.4773, valid loss 31.8511\n","Epoch 0010: train loss 31.3516, valid loss 31.5310\n","Epoch 0011: train loss 31.5010, valid loss 31.5569\n","Epoch 0012: train loss 31.2306, valid loss 31.2432\n","Epoch 0013: train loss 31.1462, valid loss 31.6721\n","Epoch 0014: train loss 31.1812, valid loss 31.7711\n","Epoch 0015: train loss 31.0242, valid loss 31.4930\n","Epoch 0016: train loss 31.1513, valid loss 31.3776\n","Epoch 0017: train loss 31.2053, valid loss 31.4669\n","Epoch 0018: train loss 31.0978, valid loss 31.2924\n","Epoch 0019: train loss 30.8832, valid loss 30.9737\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:39:24,672] Trial 13 finished with value: 31.111247016249525 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'num_layers': 1, 'learning_rate': 3.794475404097717e-05, 'batch_size': 16}. Best is trial 11 with value: 31.02073034365079.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.7477, valid loss 31.1112\n","Finished Training.\n","Epoch 0001: train loss 756.9808, valid loss 341.9909\n","Epoch 0002: train loss 281.4757, valid loss 219.3424\n","Epoch 0003: train loss 201.8092, valid loss 186.9243\n","Epoch 0004: train loss 174.6439, valid loss 162.7407\n","Epoch 0005: train loss 147.9408, valid loss 138.2681\n","Epoch 0006: train loss 133.0263, valid loss 131.7087\n","Epoch 0007: train loss 129.0760, valid loss 128.9473\n","Epoch 0008: train loss 127.4119, valid loss 126.9928\n","Epoch 0009: train loss 126.3877, valid loss 125.5933\n","Epoch 0010: train loss 124.9944, valid loss 126.5077\n","Epoch 0011: train loss 125.1953, valid loss 127.2320\n","Epoch 0012: train loss 124.4192, valid loss 125.3496\n","Epoch 0013: train loss 123.9187, valid loss 124.5233\n","Epoch 0014: train loss 123.6656, valid loss 125.8481\n","Epoch 0015: train loss 123.5783, valid loss 124.0735\n","Epoch 0016: train loss 123.1198, valid loss 123.6645\n","Epoch 0017: train loss 123.1555, valid loss 124.1432\n","Epoch 0018: train loss 122.9218, valid loss 124.7750\n","Epoch 0019: train loss 123.1604, valid loss 124.8681\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:47:18,291] Trial 14 finished with value: 123.55433452663137 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'num_layers': 2, 'learning_rate': 4.879173085543844e-05, 'batch_size': 64}. Best is trial 11 with value: 31.02073034365079.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 122.8030, valid loss 123.5543\n","Finished Training.\n","Epoch 0001: train loss 117.2788, valid loss 50.9300\n","Epoch 0002: train loss 43.3982, valid loss 36.8685\n","Epoch 0003: train loss 34.8989, valid loss 33.8330\n","Epoch 0004: train loss 33.0946, valid loss 32.6588\n","Epoch 0005: train loss 32.4290, valid loss 33.0966\n","Epoch 0006: train loss 31.7896, valid loss 32.6363\n","Epoch 0007: train loss 31.5518, valid loss 31.8426\n","Epoch 0008: train loss 31.3887, valid loss 31.6930\n","Epoch 0009: train loss 31.2832, valid loss 31.4824\n","Epoch 0010: train loss 31.2602, valid loss 31.9183\n","Epoch 0011: train loss 31.2912, valid loss 31.6214\n","Epoch 0012: train loss 31.0014, valid loss 31.3443\n","Epoch 0013: train loss 31.2256, valid loss 31.7299\n","Epoch 0014: train loss 31.1107, valid loss 31.4203\n","Epoch 0015: train loss 31.2008, valid loss 31.2533\n","Epoch 0016: train loss 30.9944, valid loss 31.4932\n","Epoch 0017: train loss 31.0551, valid loss 31.3766\n","Epoch 0018: train loss 31.1082, valid loss 31.2514\n","Epoch 0019: train loss 31.0392, valid loss 31.6833\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:52:05,709] Trial 15 finished with value: 31.24706961778219 and parameters: {'hidden_dim': 256, 'latent_dim': 32, 'num_layers': 1, 'learning_rate': 4.812175916750923e-05, 'batch_size': 16}. Best is trial 11 with value: 31.02073034365079.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.9413, valid loss 31.2471\n","Finished Training.\n","Epoch 0001: train loss 78.9197, valid loss 40.3807\n","Epoch 0002: train loss 36.3278, valid loss 34.0748\n","Epoch 0003: train loss 32.1212, valid loss 32.1515\n","Epoch 0004: train loss 31.4827, valid loss 31.4957\n","Epoch 0005: train loss 31.5754, valid loss 31.6256\n","Epoch 0006: train loss 31.4158, valid loss 31.5530\n","Epoch 0007: train loss 31.2616, valid loss 32.6618\n","Epoch 0008: train loss 31.3977, valid loss 31.4350\n","Epoch 0009: train loss 31.1993, valid loss 31.7185\n","Epoch 0010: train loss 31.0872, valid loss 31.2553\n","Epoch 0011: train loss 31.1799, valid loss 31.7283\n","Epoch 0012: train loss 30.9393, valid loss 31.1969\n","Epoch 0013: train loss 30.9848, valid loss 31.1861\n","Epoch 0014: train loss 31.0329, valid loss 31.3273\n","Epoch 0015: train loss 31.0279, valid loss 31.5616\n","Epoch 0016: train loss 30.8375, valid loss 31.4361\n","Epoch 0017: train loss 30.9466, valid loss 31.1924\n","Epoch 0018: train loss 30.9469, valid loss 31.0292\n","Epoch 0019: train loss 30.9403, valid loss 31.0195\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:54:02,325] Trial 16 finished with value: 30.996924469979962 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 1, 'learning_rate': 0.0004312295723677129, 'batch_size': 16}. Best is trial 16 with value: 30.996924469979962.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.7303, valid loss 30.9969\n","Finished Training.\n","Epoch 0001: train loss 64.4553, valid loss 39.5907\n","Epoch 0002: train loss 36.0945, valid loss 32.5487\n","Epoch 0003: train loss 31.6797, valid loss 32.0005\n","Epoch 0004: train loss 31.5945, valid loss 31.5230\n","Epoch 0005: train loss 31.3028, valid loss 31.5599\n","Epoch 0006: train loss 31.2416, valid loss 31.3869\n","Epoch 0007: train loss 31.1454, valid loss 31.2677\n","Epoch 0008: train loss 31.0121, valid loss 31.7783\n","Epoch 0009: train loss 31.0321, valid loss 31.4504\n","Epoch 0010: train loss 30.9876, valid loss 31.2508\n","Epoch 0011: train loss 31.0047, valid loss 30.7389\n","Epoch 0012: train loss 30.9470, valid loss 31.1894\n","Epoch 0013: train loss 30.9619, valid loss 31.1161\n","Epoch 0014: train loss 30.8325, valid loss 31.5326\n","Epoch 0015: train loss 30.9766, valid loss 31.2592\n","Epoch 0016: train loss 30.8767, valid loss 30.9342\n","Epoch 0017: train loss 30.8579, valid loss 31.1476\n","Epoch 0018: train loss 30.5713, valid loss 30.7849\n","Epoch 0019: train loss 30.4192, valid loss 30.7243\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 16:58:17,855] Trial 17 finished with value: 30.691591984323793 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0003887818990651667, 'batch_size': 16}. Best is trial 17 with value: 30.691591984323793.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.4341, valid loss 30.6916\n","Finished Training.\n","Epoch 0001: train loss 469.9267, valid loss 249.3795\n","Epoch 0002: train loss 205.6554, valid loss 164.8463\n","Epoch 0003: train loss 158.0338, valid loss 156.5345\n","Epoch 0004: train loss 138.3824, valid loss 128.8392\n","Epoch 0005: train loss 126.6792, valid loss 128.7142\n","Epoch 0006: train loss 125.4891, valid loss 126.3415\n","Epoch 0007: train loss 125.3064, valid loss 126.0968\n","Epoch 0008: train loss 124.6708, valid loss 126.5625\n","Epoch 0009: train loss 124.3858, valid loss 124.6155\n","Epoch 0010: train loss 124.3138, valid loss 123.8996\n","Epoch 0011: train loss 123.4051, valid loss 126.6214\n","Epoch 0012: train loss 123.1544, valid loss 123.7137\n","Epoch 0013: train loss 123.2275, valid loss 123.3400\n","Epoch 0014: train loss 123.1000, valid loss 123.7893\n","Epoch 0015: train loss 122.7188, valid loss 124.5789\n","Epoch 0016: train loss 122.6542, valid loss 123.7148\n","Epoch 0017: train loss 122.7005, valid loss 125.2829\n","Epoch 0018: train loss 122.8170, valid loss 125.2622\n","Epoch 0019: train loss 123.0984, valid loss 122.9584\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:00:06,495] Trial 18 finished with value: 123.02374666128586 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0004007748233600144, 'batch_size': 64}. Best is trial 17 with value: 30.691591984323793.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 123.0623, valid loss 123.0237\n","Finished Training.\n","Epoch 0001: train loss 193.7570, valid loss 83.5838\n","Epoch 0002: train loss 80.2253, valid loss 79.4292\n","Epoch 0003: train loss 70.5084, valid loss 64.9970\n","Epoch 0004: train loss 63.7151, valid loss 63.3015\n","Epoch 0005: train loss 62.8125, valid loss 64.1044\n","Epoch 0006: train loss 62.4130, valid loss 63.0729\n","Epoch 0007: train loss 62.2025, valid loss 63.5589\n","Epoch 0008: train loss 62.1231, valid loss 62.7914\n","Epoch 0009: train loss 62.2467, valid loss 61.9149\n","Epoch 0010: train loss 61.9524, valid loss 63.8790\n","Epoch 0011: train loss 61.6832, valid loss 61.9674\n","Epoch 0012: train loss 61.8583, valid loss 63.0116\n","Epoch 0013: train loss 61.8577, valid loss 62.3068\n","Epoch 0014: train loss 61.7052, valid loss 61.6643\n","Epoch 0015: train loss 61.6416, valid loss 62.8626\n","Epoch 0016: train loss 61.9049, valid loss 62.7633\n","Epoch 0017: train loss 61.7245, valid loss 62.0770\n","Epoch 0018: train loss 61.8451, valid loss 65.7513\n","Epoch 0019: train loss 61.6704, valid loss 62.1195\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:02:43,917] Trial 19 finished with value: 62.29298182283895 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00040743511434019393, 'batch_size': 32}. Best is trial 17 with value: 30.691591984323793.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 61.5337, valid loss 62.2930\n","Finished Training.\n","Epoch 0001: train loss 69.3991, valid loss 33.3293\n","Epoch 0002: train loss 32.2189, valid loss 31.9802\n","Epoch 0003: train loss 31.7615, valid loss 31.7286\n","Epoch 0004: train loss 31.5282, valid loss 31.7172\n","Epoch 0005: train loss 31.5269, valid loss 31.1763\n","Epoch 0006: train loss 31.1172, valid loss 31.3569\n","Epoch 0007: train loss 31.1711, valid loss 31.3341\n","Epoch 0008: train loss 31.0836, valid loss 31.4422\n","Epoch 0009: train loss 30.9775, valid loss 31.5317\n","Epoch 0010: train loss 31.0464, valid loss 31.3286\n","Epoch 0011: train loss 31.1356, valid loss 31.1850\n","Epoch 0012: train loss 30.3966, valid loss 31.0035\n","Epoch 0013: train loss 30.4345, valid loss 30.8084\n","Epoch 0014: train loss 30.5078, valid loss 30.9006\n","Epoch 0015: train loss 30.4293, valid loss 30.6780\n","Epoch 0016: train loss 30.4714, valid loss 30.9312\n","Epoch 0017: train loss 30.4933, valid loss 30.5304\n","Epoch 0018: train loss 30.4470, valid loss 30.8583\n","Epoch 0019: train loss 30.4158, valid loss 30.7690\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:06:57,652] Trial 20 finished with value: 30.89776111274176 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00031815465111307694, 'batch_size': 16}. Best is trial 17 with value: 30.691591984323793.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.5133, valid loss 30.8978\n","Finished Training.\n","Epoch 0001: train loss 72.3757, valid loss 38.8580\n","Epoch 0002: train loss 35.3077, valid loss 32.2403\n","Epoch 0003: train loss 31.8880, valid loss 32.2444\n","Epoch 0004: train loss 31.4428, valid loss 31.6181\n","Epoch 0005: train loss 31.4157, valid loss 31.4891\n","Epoch 0006: train loss 31.2019, valid loss 32.0926\n","Epoch 0007: train loss 31.1862, valid loss 31.7143\n","Epoch 0008: train loss 31.0295, valid loss 31.6370\n","Epoch 0009: train loss 31.0773, valid loss 31.2812\n","Epoch 0010: train loss 30.8606, valid loss 31.1998\n","Epoch 0011: train loss 30.8997, valid loss 31.1965\n","Epoch 0012: train loss 30.9911, valid loss 31.0700\n","Epoch 0013: train loss 30.8838, valid loss 30.9899\n","Epoch 0014: train loss 31.0667, valid loss 31.1729\n","Epoch 0015: train loss 30.8582, valid loss 31.3491\n","Epoch 0016: train loss 30.7509, valid loss 31.0073\n","Epoch 0017: train loss 31.0568, valid loss 31.3402\n","Epoch 0018: train loss 30.9183, valid loss 30.8424\n","Epoch 0019: train loss 30.7662, valid loss 30.9640\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:11:12,926] Trial 21 finished with value: 30.968140778916606 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00035894369151069394, 'batch_size': 16}. Best is trial 17 with value: 30.691591984323793.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8663, valid loss 30.9681\n","Finished Training.\n","Epoch 0001: train loss 95.5712, valid loss 42.9968\n","Epoch 0002: train loss 37.4363, valid loss 32.8670\n","Epoch 0003: train loss 32.2846, valid loss 32.0292\n","Epoch 0004: train loss 31.8953, valid loss 31.5768\n","Epoch 0005: train loss 31.3968, valid loss 31.3629\n","Epoch 0006: train loss 31.2461, valid loss 31.3770\n","Epoch 0007: train loss 31.1009, valid loss 31.3777\n","Epoch 0008: train loss 31.1862, valid loss 31.2439\n","Epoch 0009: train loss 31.0162, valid loss 31.3210\n","Epoch 0010: train loss 30.8671, valid loss 31.3985\n","Epoch 0011: train loss 30.7336, valid loss 31.1910\n","Epoch 0012: train loss 30.9523, valid loss 30.9188\n","Epoch 0013: train loss 30.9689, valid loss 30.9760\n","Epoch 0014: train loss 30.7872, valid loss 31.3071\n","Epoch 0015: train loss 30.8733, valid loss 31.1112\n","Epoch 0016: train loss 30.7733, valid loss 30.9439\n","Epoch 0017: train loss 30.8164, valid loss 30.9770\n","Epoch 0018: train loss 30.8342, valid loss 31.0106\n","Epoch 0019: train loss 30.4007, valid loss 30.8104\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:15:25,802] Trial 22 finished with value: 31.050006273533967 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00016762470812852582, 'batch_size': 16}. Best is trial 17 with value: 30.691591984323793.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.4264, valid loss 31.0500\n","Finished Training.\n","Epoch 0001: train loss 81.4330, valid loss 41.2378\n","Epoch 0002: train loss 36.6270, valid loss 32.3857\n","Epoch 0003: train loss 31.9179, valid loss 31.5339\n","Epoch 0004: train loss 31.5162, valid loss 32.3022\n","Epoch 0005: train loss 31.2258, valid loss 31.6436\n","Epoch 0006: train loss 31.0514, valid loss 31.2026\n","Epoch 0007: train loss 31.2243, valid loss 31.0988\n","Epoch 0008: train loss 31.0628, valid loss 31.4534\n","Epoch 0009: train loss 31.0715, valid loss 31.4259\n","Epoch 0010: train loss 31.0470, valid loss 31.4805\n","Epoch 0011: train loss 30.9515, valid loss 31.4299\n","Epoch 0012: train loss 30.8443, valid loss 31.4050\n","Epoch 0013: train loss 30.7634, valid loss 31.1160\n","Epoch 0014: train loss 30.4870, valid loss 30.5091\n","Epoch 0015: train loss 30.3709, valid loss 30.8576\n","Epoch 0016: train loss 30.5043, valid loss 30.8658\n","Epoch 0017: train loss 30.5328, valid loss 30.7780\n","Epoch 0018: train loss 30.5551, valid loss 30.7963\n","Epoch 0019: train loss 30.5641, valid loss 30.6938\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:19:38,814] Trial 23 finished with value: 30.6621642380618 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00027582796680544796, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.2831, valid loss 30.6622\n","Finished Training.\n","Epoch 0001: train loss 87.8011, valid loss 41.3142\n","Epoch 0002: train loss 36.8263, valid loss 33.5617\n","Epoch 0003: train loss 31.9937, valid loss 31.8995\n","Epoch 0004: train loss 31.7512, valid loss 32.2972\n","Epoch 0005: train loss 31.2690, valid loss 31.1988\n","Epoch 0006: train loss 31.2737, valid loss 31.0974\n","Epoch 0007: train loss 30.9810, valid loss 31.1255\n","Epoch 0008: train loss 31.0705, valid loss 31.2876\n","Epoch 0009: train loss 31.0133, valid loss 31.3473\n","Epoch 0010: train loss 31.0300, valid loss 31.0019\n","Epoch 0011: train loss 30.8988, valid loss 31.2163\n","Epoch 0012: train loss 31.0203, valid loss 31.2266\n","Epoch 0013: train loss 30.8908, valid loss 31.1765\n","Epoch 0014: train loss 30.8129, valid loss 31.1335\n","Epoch 0015: train loss 30.8095, valid loss 31.1724\n","Epoch 0016: train loss 30.8496, valid loss 31.1991\n","Epoch 0017: train loss 30.3187, valid loss 30.6500\n","Epoch 0018: train loss 30.4051, valid loss 30.8223\n","Epoch 0019: train loss 30.5942, valid loss 30.8022\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:23:51,344] Trial 24 finished with value: 30.91177083133312 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00024696392191390627, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.3341, valid loss 30.9118\n","Finished Training.\n","Epoch 0001: train loss 120.6235, valid loss 51.3631\n","Epoch 0002: train loss 44.1217, valid loss 39.8308\n","Epoch 0003: train loss 34.9375, valid loss 32.9917\n","Epoch 0004: train loss 32.1051, valid loss 32.0244\n","Epoch 0005: train loss 31.7691, valid loss 31.7710\n","Epoch 0006: train loss 31.3576, valid loss 31.6569\n","Epoch 0007: train loss 31.3674, valid loss 31.6437\n","Epoch 0008: train loss 31.1671, valid loss 31.5849\n","Epoch 0009: train loss 31.2138, valid loss 31.5934\n","Epoch 0010: train loss 30.9821, valid loss 31.5820\n","Epoch 0011: train loss 31.0078, valid loss 31.1772\n","Epoch 0012: train loss 30.8919, valid loss 31.0359\n","Epoch 0013: train loss 30.9800, valid loss 30.9197\n","Epoch 0014: train loss 30.8250, valid loss 30.9905\n","Epoch 0015: train loss 30.9187, valid loss 31.2187\n","Epoch 0016: train loss 30.8156, valid loss 31.1666\n","Epoch 0017: train loss 30.6776, valid loss 30.9448\n","Epoch 0018: train loss 30.7483, valid loss 31.1355\n","Epoch 0019: train loss 30.8589, valid loss 31.3404\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:28:07,315] Trial 25 finished with value: 30.794692718134392 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 9.679023842653576e-05, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.3990, valid loss 30.7947\n","Finished Training.\n","Epoch 0001: train loss 135.1826, valid loss 55.4220\n","Epoch 0002: train loss 46.7306, valid loss 41.4503\n","Epoch 0003: train loss 37.0372, valid loss 33.7365\n","Epoch 0004: train loss 32.5641, valid loss 32.3528\n","Epoch 0005: train loss 31.9222, valid loss 32.2548\n","Epoch 0006: train loss 31.5619, valid loss 32.0105\n","Epoch 0007: train loss 31.4412, valid loss 32.2835\n","Epoch 0008: train loss 31.4527, valid loss 31.3806\n","Epoch 0009: train loss 31.2030, valid loss 31.2481\n","Epoch 0010: train loss 30.8967, valid loss 31.2426\n","Epoch 0011: train loss 30.9996, valid loss 31.3395\n","Epoch 0012: train loss 30.9688, valid loss 31.4481\n","Epoch 0013: train loss 30.9807, valid loss 31.1253\n","Epoch 0014: train loss 30.9172, valid loss 31.5101\n","Epoch 0015: train loss 30.7721, valid loss 31.2123\n","Epoch 0016: train loss 30.7142, valid loss 31.0267\n","Epoch 0017: train loss 30.8707, valid loss 31.4787\n","Epoch 0018: train loss 30.7345, valid loss 30.9399\n","Epoch 0019: train loss 30.5589, valid loss 30.7306\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:32:22,418] Trial 26 finished with value: 30.937660828065336 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 8.131605414340248e-05, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8831, valid loss 30.9377\n","Finished Training.\n","Epoch 0001: train loss 114.7518, valid loss 50.3112\n","Epoch 0002: train loss 44.0675, valid loss 40.8084\n","Epoch 0003: train loss 35.3478, valid loss 33.1732\n","Epoch 0004: train loss 32.0687, valid loss 32.4475\n","Epoch 0005: train loss 31.7821, valid loss 31.8463\n","Epoch 0006: train loss 31.4993, valid loss 31.4553\n","Epoch 0007: train loss 31.3920, valid loss 31.2338\n","Epoch 0008: train loss 31.2802, valid loss 31.6201\n","Epoch 0009: train loss 31.0892, valid loss 31.1716\n","Epoch 0010: train loss 31.1470, valid loss 31.0134\n","Epoch 0011: train loss 30.9802, valid loss 30.9471\n","Epoch 0012: train loss 30.8550, valid loss 31.0389\n","Epoch 0013: train loss 30.8456, valid loss 30.9475\n","Epoch 0014: train loss 30.8766, valid loss 31.2644\n","Epoch 0015: train loss 30.8884, valid loss 31.3811\n","Epoch 0016: train loss 30.8205, valid loss 31.1562\n","Epoch 0017: train loss 30.8873, valid loss 30.9640\n","Epoch 0018: train loss 30.5819, valid loss 30.9412\n","Epoch 0019: train loss 30.4961, valid loss 30.7541\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:36:35,218] Trial 27 finished with value: 30.84631194246842 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 9.825248264776935e-05, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.4702, valid loss 30.8463\n","Finished Training.\n","Epoch 0001: train loss 453.0919, valid loss 170.2545\n","Epoch 0002: train loss 127.3579, valid loss 100.6436\n","Epoch 0003: train loss 87.2654, valid loss 78.8664\n","Epoch 0004: train loss 72.6132, valid loss 69.8672\n","Epoch 0005: train loss 67.2887, valid loss 66.3126\n","Epoch 0006: train loss 64.4673, valid loss 64.0842\n","Epoch 0007: train loss 63.5053, valid loss 63.6862\n","Epoch 0008: train loss 62.7754, valid loss 63.3432\n","Epoch 0009: train loss 62.4794, valid loss 63.2320\n","Epoch 0010: train loss 62.1336, valid loss 63.1186\n","Epoch 0011: train loss 62.3762, valid loss 62.6437\n","Epoch 0012: train loss 62.0672, valid loss 62.6832\n","Epoch 0013: train loss 61.9684, valid loss 63.1617\n","Epoch 0014: train loss 61.6356, valid loss 62.4587\n","Epoch 0015: train loss 61.7596, valid loss 62.4471\n","Epoch 0016: train loss 61.6158, valid loss 62.5602\n","Epoch 0017: train loss 61.6948, valid loss 62.1158\n","Epoch 0018: train loss 61.3937, valid loss 61.6158\n","Epoch 0019: train loss 61.5405, valid loss 62.6308\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:38:26,303] Trial 28 finished with value: 62.86483980385998 and parameters: {'hidden_dim': 64, 'latent_dim': 16, 'num_layers': 2, 'learning_rate': 7.403515763047823e-05, 'batch_size': 32}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 61.4692, valid loss 62.8648\n","Finished Training.\n","Epoch 0001: train loss 900.7700, valid loss 349.3777\n","Epoch 0002: train loss 304.8035, valid loss 265.2157\n","Epoch 0003: train loss 206.3703, valid loss 181.8269\n","Epoch 0004: train loss 170.2018, valid loss 164.2280\n","Epoch 0005: train loss 156.7964, valid loss 155.2741\n","Epoch 0006: train loss 147.7657, valid loss 144.9053\n","Epoch 0007: train loss 144.0741, valid loss 144.1836\n","Epoch 0008: train loss 143.3638, valid loss 143.4712\n","Epoch 0009: train loss 142.7450, valid loss 142.4286\n","Epoch 0010: train loss 139.1097, valid loss 133.2847\n","Epoch 0011: train loss 128.7875, valid loss 127.2586\n","Epoch 0012: train loss 126.0848, valid loss 127.8476\n","Epoch 0013: train loss 124.9548, valid loss 127.3789\n","Epoch 0014: train loss 124.8761, valid loss 126.1275\n","Epoch 0015: train loss 124.3586, valid loss 125.9773\n","Epoch 0016: train loss 125.0696, valid loss 124.6151\n","Epoch 0017: train loss 123.7110, valid loss 123.7568\n","Epoch 0018: train loss 123.2200, valid loss 124.2519\n","Epoch 0019: train loss 122.8461, valid loss 124.9171\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:40:14,306] Trial 29 finished with value: 124.91741103556619 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00015279965911165275, 'batch_size': 64}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 123.8839, valid loss 124.9174\n","Finished Training.\n","Epoch 0001: train loss 161.6159, valid loss 79.8228\n","Epoch 0002: train loss 76.8648, valid loss 75.3758\n","Epoch 0003: train loss 72.4892, valid loss 70.2349\n","Epoch 0004: train loss 62.9786, valid loss 64.7778\n","Epoch 0005: train loss 62.7768, valid loss 63.3877\n","Epoch 0006: train loss 62.0759, valid loss 62.8578\n","Epoch 0007: train loss 61.8051, valid loss 62.9925\n","Epoch 0008: train loss 62.1223, valid loss 62.7901\n","Epoch 0009: train loss 61.9746, valid loss 62.7628\n","Epoch 0010: train loss 62.0216, valid loss 62.9736\n","Epoch 0011: train loss 61.9310, valid loss 62.5313\n","Epoch 0012: train loss 61.8335, valid loss 63.5957\n","Epoch 0013: train loss 61.9216, valid loss 62.5881\n","Epoch 0014: train loss 61.6652, valid loss 62.5180\n","Epoch 0015: train loss 61.5574, valid loss 61.7375\n","Epoch 0016: train loss 61.5132, valid loss 62.6990\n","Epoch 0017: train loss 61.8161, valid loss 62.8591\n","Epoch 0018: train loss 61.8181, valid loss 61.7648\n","Epoch 0019: train loss 61.7571, valid loss 61.7975\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:42:06,096] Trial 30 finished with value: 62.387460801485325 and parameters: {'hidden_dim': 64, 'latent_dim': 16, 'num_layers': 2, 'learning_rate': 0.0005551860309019168, 'batch_size': 32}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 61.5050, valid loss 62.3875\n","Finished Training.\n","Epoch 0001: train loss 107.9558, valid loss 47.4061\n","Epoch 0002: train loss 43.0229, valid loss 40.5002\n","Epoch 0003: train loss 39.0645, valid loss 37.1367\n","Epoch 0004: train loss 33.6141, valid loss 32.3405\n","Epoch 0005: train loss 31.8771, valid loss 31.7019\n","Epoch 0006: train loss 31.3587, valid loss 31.5569\n","Epoch 0007: train loss 31.2730, valid loss 31.4400\n","Epoch 0008: train loss 31.1603, valid loss 31.3869\n","Epoch 0009: train loss 31.0713, valid loss 31.1674\n","Epoch 0010: train loss 31.1471, valid loss 31.5730\n","Epoch 0011: train loss 31.0009, valid loss 31.0354\n","Epoch 0012: train loss 30.9799, valid loss 31.0961\n","Epoch 0013: train loss 30.8043, valid loss 30.9946\n","Epoch 0014: train loss 30.9176, valid loss 31.1280\n","Epoch 0015: train loss 30.9066, valid loss 31.3148\n","Epoch 0016: train loss 30.9459, valid loss 30.9841\n","Epoch 0017: train loss 30.7058, valid loss 31.0466\n","Epoch 0018: train loss 30.9182, valid loss 31.6509\n","Epoch 0019: train loss 30.7047, valid loss 31.1177\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:46:18,907] Trial 31 finished with value: 31.087373237038374 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00011118846609203441, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8930, valid loss 31.0874\n","Finished Training.\n","Epoch 0001: train loss 135.5350, valid loss 55.9989\n","Epoch 0002: train loss 48.1192, valid loss 42.5339\n","Epoch 0003: train loss 38.7951, valid loss 34.4461\n","Epoch 0004: train loss 33.1579, valid loss 33.0768\n","Epoch 0005: train loss 32.2307, valid loss 32.0600\n","Epoch 0006: train loss 31.7400, valid loss 32.0165\n","Epoch 0007: train loss 31.2700, valid loss 31.4381\n","Epoch 0008: train loss 31.2291, valid loss 31.7105\n","Epoch 0009: train loss 31.1467, valid loss 31.8116\n","Epoch 0010: train loss 31.1601, valid loss 31.3406\n","Epoch 0011: train loss 31.0009, valid loss 31.8268\n","Epoch 0012: train loss 30.9094, valid loss 31.2097\n","Epoch 0013: train loss 31.0642, valid loss 31.2035\n","Epoch 0014: train loss 30.9359, valid loss 31.2940\n","Epoch 0015: train loss 30.6909, valid loss 31.0333\n","Epoch 0016: train loss 30.7803, valid loss 30.9999\n","Epoch 0017: train loss 31.0479, valid loss 31.0665\n","Epoch 0018: train loss 30.7652, valid loss 30.7852\n","Epoch 0019: train loss 30.7713, valid loss 30.8344\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:50:31,226] Trial 32 finished with value: 31.028954332687434 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 7.085087695031358e-05, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.7597, valid loss 31.0290\n","Finished Training.\n","Epoch 0001: train loss 72.4893, valid loss 37.6307\n","Epoch 0002: train loss 33.1127, valid loss 32.9945\n","Epoch 0003: train loss 31.6784, valid loss 31.6932\n","Epoch 0004: train loss 31.2408, valid loss 31.4795\n","Epoch 0005: train loss 31.3532, valid loss 31.3834\n","Epoch 0006: train loss 31.3588, valid loss 31.4774\n","Epoch 0007: train loss 31.0854, valid loss 31.2462\n","Epoch 0008: train loss 31.0142, valid loss 30.9842\n","Epoch 0009: train loss 30.9502, valid loss 31.2725\n","Epoch 0010: train loss 31.0142, valid loss 31.0327\n","Epoch 0011: train loss 30.9191, valid loss 31.1874\n","Epoch 0012: train loss 30.9848, valid loss 31.0351\n","Epoch 0013: train loss 31.0706, valid loss 31.1918\n","Epoch 0014: train loss 30.9347, valid loss 31.2682\n","Epoch 0015: train loss 30.4187, valid loss 30.6651\n","Epoch 0016: train loss 30.3620, valid loss 30.3393\n","Epoch 0017: train loss 30.3537, valid loss 31.0006\n","Epoch 0018: train loss 30.3427, valid loss 30.5904\n","Epoch 0019: train loss 30.4895, valid loss 30.6915\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:54:45,764] Trial 33 finished with value: 30.690360640765128 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0002580354567825295, 'batch_size': 16}. Best is trial 23 with value: 30.6621642380618.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.3678, valid loss 30.6904\n","Finished Training.\n","Epoch 0001: train loss 82.8179, valid loss 42.3302\n","Epoch 0002: train loss 39.3179, valid loss 38.4113\n","Epoch 0003: train loss 36.3471, valid loss 35.0632\n","Epoch 0004: train loss 32.1827, valid loss 31.9335\n","Epoch 0005: train loss 31.4542, valid loss 31.3835\n","Epoch 0006: train loss 31.2274, valid loss 31.7168\n","Epoch 0007: train loss 31.1783, valid loss 31.8824\n","Epoch 0008: train loss 31.0118, valid loss 31.5384\n","Epoch 0009: train loss 31.1795, valid loss 31.2585\n","Epoch 0010: train loss 31.1832, valid loss 30.8501\n","Epoch 0011: train loss 31.0104, valid loss 31.4440\n","Epoch 0012: train loss 30.9661, valid loss 31.0701\n","Epoch 0013: train loss 30.9341, valid loss 31.2980\n","Epoch 0014: train loss 30.8683, valid loss 31.2028\n","Epoch 0015: train loss 30.9700, valid loss 30.9535\n","Epoch 0016: train loss 30.7578, valid loss 31.4168\n","Epoch 0017: train loss 30.4738, valid loss 30.8151\n","Epoch 0018: train loss 30.6250, valid loss 30.8043\n","Epoch 0019: train loss 30.5069, valid loss 30.5344\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 17:59:01,993] Trial 34 finished with value: 30.610877956790425 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0002398370047454381, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.3117, valid loss 30.6109\n","Finished Training.\n","Epoch 0001: train loss 65.2336, valid loss 33.7369\n","Epoch 0002: train loss 32.2115, valid loss 32.3598\n","Epoch 0003: train loss 31.5736, valid loss 31.7574\n","Epoch 0004: train loss 31.5064, valid loss 31.5689\n","Epoch 0005: train loss 31.2245, valid loss 32.3923\n","Epoch 0006: train loss 31.4308, valid loss 31.6885\n","Epoch 0007: train loss 31.2323, valid loss 32.0394\n","Epoch 0008: train loss 31.0652, valid loss 31.8869\n","Epoch 0009: train loss 31.0602, valid loss 31.3677\n","Epoch 0010: train loss 31.0456, valid loss 31.1198\n","Epoch 0011: train loss 30.8188, valid loss 31.4354\n","Epoch 0012: train loss 31.0650, valid loss 31.1672\n","Epoch 0013: train loss 30.9062, valid loss 31.5075\n","Epoch 0014: train loss 30.8784, valid loss 30.9266\n","Epoch 0015: train loss 30.8754, valid loss 30.9074\n","Epoch 0016: train loss 30.9372, valid loss 31.4064\n","Epoch 0017: train loss 30.8439, valid loss 31.3241\n","Epoch 0018: train loss 30.9579, valid loss 31.3787\n","Epoch 0019: train loss 30.7514, valid loss 30.8995\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:03:26,477] Trial 35 finished with value: 31.432681890909144 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'num_layers': 2, 'learning_rate': 0.00024605073990136185, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8991, valid loss 31.4327\n","Finished Training.\n","Epoch 0001: train loss 156.8298, valid loss 80.3367\n","Epoch 0002: train loss 79.8855, valid loss 78.4969\n","Epoch 0003: train loss 68.8015, valid loss 64.8920\n","Epoch 0004: train loss 63.3653, valid loss 64.3212\n","Epoch 0005: train loss 62.6820, valid loss 62.6259\n","Epoch 0006: train loss 62.7672, valid loss 63.6486\n","Epoch 0007: train loss 62.0272, valid loss 63.2554\n","Epoch 0008: train loss 62.2881, valid loss 63.3462\n","Epoch 0009: train loss 62.0921, valid loss 62.7711\n","Epoch 0010: train loss 62.2687, valid loss 62.1093\n","Epoch 0011: train loss 62.2867, valid loss 63.1100\n","Epoch 0012: train loss 62.4121, valid loss 62.8164\n","Epoch 0013: train loss 61.9983, valid loss 62.5305\n","Epoch 0014: train loss 61.9653, valid loss 62.6683\n","Epoch 0015: train loss 61.9565, valid loss 63.8572\n","Epoch 0016: train loss 62.0096, valid loss 61.6865\n","Epoch 0017: train loss 61.5023, valid loss 63.2094\n","Epoch 0018: train loss 61.8147, valid loss 63.3485\n","Epoch 0019: train loss 61.4186, valid loss 62.3572\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:06:01,105] Trial 36 finished with value: 62.28639308343666 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'num_layers': 3, 'learning_rate': 0.0005312181736622203, 'batch_size': 32}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 62.0076, valid loss 62.2864\n","Finished Training.\n","Epoch 0001: train loss 79.6982, valid loss 39.3505\n","Epoch 0002: train loss 33.7988, valid loss 32.5061\n","Epoch 0003: train loss 31.8142, valid loss 31.2076\n","Epoch 0004: train loss 31.3768, valid loss 31.4159\n","Epoch 0005: train loss 31.2251, valid loss 32.3431\n","Epoch 0006: train loss 31.2994, valid loss 31.3567\n","Epoch 0007: train loss 31.2329, valid loss 30.9256\n","Epoch 0008: train loss 31.0609, valid loss 31.1381\n","Epoch 0009: train loss 30.9797, valid loss 31.1725\n","Epoch 0010: train loss 30.9536, valid loss 31.2023\n","Epoch 0011: train loss 31.0571, valid loss 30.8353\n","Epoch 0012: train loss 31.0004, valid loss 31.3270\n","Epoch 0013: train loss 31.0644, valid loss 30.8311\n","Epoch 0014: train loss 31.0286, valid loss 31.0009\n","Epoch 0015: train loss 30.9111, valid loss 31.3042\n","Epoch 0016: train loss 30.9143, valid loss 31.3966\n","Epoch 0017: train loss 30.8119, valid loss 31.6670\n","Epoch 0018: train loss 30.9027, valid loss 31.6776\n","Epoch 0019: train loss 30.7885, valid loss 31.2675\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:10:12,744] Trial 37 finished with value: 30.915298958396196 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0002784169045276934, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.4158, valid loss 30.9153\n","Finished Training.\n","Epoch 0001: train loss 87.7095, valid loss 41.6361\n","Epoch 0002: train loss 39.8298, valid loss 38.5629\n","Epoch 0003: train loss 33.7211, valid loss 31.9281\n","Epoch 0004: train loss 31.7518, valid loss 31.6600\n","Epoch 0005: train loss 31.4324, valid loss 32.0938\n","Epoch 0006: train loss 31.2633, valid loss 31.3387\n","Epoch 0007: train loss 31.0874, valid loss 31.9138\n","Epoch 0008: train loss 31.1814, valid loss 31.6800\n","Epoch 0009: train loss 31.2382, valid loss 31.2896\n","Epoch 0010: train loss 31.0543, valid loss 31.4561\n","Epoch 0011: train loss 31.0572, valid loss 31.0708\n","Epoch 0012: train loss 30.9368, valid loss 30.9958\n","Epoch 0013: train loss 30.9755, valid loss 30.9577\n","Epoch 0014: train loss 31.1226, valid loss 31.0822\n","Epoch 0015: train loss 30.9263, valid loss 31.0427\n","Epoch 0016: train loss 30.8796, valid loss 30.8313\n","Epoch 0017: train loss 30.8420, valid loss 31.2540\n","Epoch 0018: train loss 30.9477, valid loss 31.0958\n","Epoch 0019: train loss 30.9216, valid loss 31.1853\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:14:33,659] Trial 38 finished with value: 31.470261591650573 and parameters: {'hidden_dim': 128, 'latent_dim': 64, 'num_layers': 2, 'learning_rate': 0.00018239440045638913, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.9289, valid loss 31.4703\n","Finished Training.\n","Epoch 0001: train loss 854.2650, valid loss 319.4533\n","Epoch 0002: train loss 282.0647, valid loss 249.9844\n","Epoch 0003: train loss 195.2969, valid loss 170.4802\n","Epoch 0004: train loss 158.0338, valid loss 144.6348\n","Epoch 0005: train loss 134.4424, valid loss 132.8193\n","Epoch 0006: train loss 129.9223, valid loss 129.8691\n","Epoch 0007: train loss 128.0151, valid loss 128.7314\n","Epoch 0008: train loss 126.9620, valid loss 127.1476\n","Epoch 0009: train loss 125.9400, valid loss 125.8345\n","Epoch 0010: train loss 125.5125, valid loss 127.6644\n","Epoch 0011: train loss 125.6059, valid loss 124.5479\n","Epoch 0012: train loss 123.6841, valid loss 125.1963\n","Epoch 0013: train loss 123.2712, valid loss 123.3318\n","Epoch 0014: train loss 123.6764, valid loss 125.3702\n","Epoch 0015: train loss 123.4992, valid loss 124.8315\n","Epoch 0016: train loss 123.1374, valid loss 124.4408\n","Epoch 0017: train loss 123.1954, valid loss 123.4059\n","Epoch 0018: train loss 122.2818, valid loss 123.0878\n","Epoch 0019: train loss 122.5555, valid loss 123.9816\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:16:21,002] Trial 39 finished with value: 124.37760876897556 and parameters: {'hidden_dim': 64, 'latent_dim': 16, 'num_layers': 3, 'learning_rate': 0.00013452215117980512, 'batch_size': 64}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 122.7767, valid loss 124.3776\n","Finished Training.\n","Epoch 0001: train loss 53.5551, valid loss 32.9193\n","Epoch 0002: train loss 32.3784, valid loss 33.1739\n","Epoch 0003: train loss 31.8296, valid loss 32.3265\n","Epoch 0004: train loss 31.6419, valid loss 31.6677\n","Epoch 0005: train loss 31.4957, valid loss 31.3014\n","Epoch 0006: train loss 31.4694, valid loss 31.8140\n","Epoch 0007: train loss 31.2363, valid loss 31.3342\n","Epoch 0008: train loss 31.3174, valid loss 31.8313\n","Epoch 0009: train loss 31.2499, valid loss 31.5441\n","Epoch 0010: train loss 31.1926, valid loss 31.2503\n","Epoch 0011: train loss 31.1763, valid loss 31.3388\n","Epoch 0012: train loss 31.1141, valid loss 31.2821\n","Epoch 0013: train loss 30.9752, valid loss 31.3904\n","Epoch 0014: train loss 31.0207, valid loss 31.0431\n","Epoch 0015: train loss 31.0950, valid loss 30.8265\n","Epoch 0016: train loss 31.0485, valid loss 31.2404\n","Epoch 0017: train loss 30.8884, valid loss 31.1805\n","Epoch 0018: train loss 30.9559, valid loss 31.5334\n","Epoch 0019: train loss 31.0372, valid loss 30.8312\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:22:33,711] Trial 40 finished with value: 30.956145324064106 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0005264066607731249, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8170, valid loss 30.9561\n","Finished Training.\n","Epoch 0001: train loss 92.5596, valid loss 41.4944\n","Epoch 0002: train loss 40.3315, valid loss 39.9637\n","Epoch 0003: train loss 37.5096, valid loss 33.0303\n","Epoch 0004: train loss 31.8461, valid loss 31.8847\n","Epoch 0005: train loss 31.4890, valid loss 31.2313\n","Epoch 0006: train loss 31.1446, valid loss 31.4187\n","Epoch 0007: train loss 31.2196, valid loss 31.2048\n","Epoch 0008: train loss 30.9521, valid loss 31.1831\n","Epoch 0009: train loss 31.0760, valid loss 30.9546\n","Epoch 0010: train loss 30.9692, valid loss 31.2512\n","Epoch 0011: train loss 30.8102, valid loss 31.4594\n","Epoch 0012: train loss 31.0965, valid loss 31.8056\n","Epoch 0013: train loss 30.8664, valid loss 30.9720\n","Epoch 0014: train loss 30.9840, valid loss 31.5902\n","Epoch 0015: train loss 30.9539, valid loss 31.2542\n","Epoch 0016: train loss 30.5150, valid loss 30.8856\n","Epoch 0017: train loss 30.4710, valid loss 30.6802\n","Epoch 0018: train loss 30.4886, valid loss 30.7047\n","Epoch 0019: train loss 30.4919, valid loss 30.6524\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:26:47,971] Trial 41 finished with value: 30.827958092707373 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00021542335965708202, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.2936, valid loss 30.8280\n","Finished Training.\n","Epoch 0001: train loss 75.3595, valid loss 41.2388\n","Epoch 0002: train loss 38.7222, valid loss 36.3908\n","Epoch 0003: train loss 32.7966, valid loss 32.8159\n","Epoch 0004: train loss 31.5692, valid loss 31.3851\n","Epoch 0005: train loss 31.1369, valid loss 31.3933\n","Epoch 0006: train loss 31.1275, valid loss 31.2030\n","Epoch 0007: train loss 31.1951, valid loss 31.0492\n","Epoch 0008: train loss 31.1796, valid loss 31.4122\n","Epoch 0009: train loss 31.0659, valid loss 31.5696\n","Epoch 0010: train loss 31.0386, valid loss 31.3423\n","Epoch 0011: train loss 31.0792, valid loss 31.3071\n","Epoch 0012: train loss 30.8871, valid loss 31.2525\n","Epoch 0013: train loss 31.0020, valid loss 31.1456\n","Epoch 0014: train loss 30.4167, valid loss 30.7771\n","Epoch 0015: train loss 30.4729, valid loss 30.7683\n","Epoch 0016: train loss 30.5037, valid loss 30.9764\n","Epoch 0017: train loss 30.5350, valid loss 30.8096\n","Epoch 0018: train loss 30.4803, valid loss 30.7950\n","Epoch 0019: train loss 30.4455, valid loss 30.6819\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:31:02,234] Trial 42 finished with value: 30.747371832529705 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0003256701117239035, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.3735, valid loss 30.7474\n","Finished Training.\n","Epoch 0001: train loss 67.6092, valid loss 33.7551\n","Epoch 0002: train loss 32.2458, valid loss 32.0939\n","Epoch 0003: train loss 31.6942, valid loss 31.4561\n","Epoch 0004: train loss 31.4845, valid loss 33.5495\n","Epoch 0005: train loss 31.2638, valid loss 31.8915\n","Epoch 0006: train loss 31.4321, valid loss 31.4200\n","Epoch 0007: train loss 30.9825, valid loss 31.5453\n","Epoch 0008: train loss 30.9810, valid loss 31.4173\n","Epoch 0009: train loss 31.2529, valid loss 31.1799\n","Epoch 0010: train loss 30.9837, valid loss 31.1278\n","Epoch 0011: train loss 30.9731, valid loss 31.3718\n","Epoch 0012: train loss 30.9390, valid loss 31.2478\n","Epoch 0013: train loss 30.9228, valid loss 30.5751\n","Epoch 0014: train loss 30.7882, valid loss 31.3378\n","Epoch 0015: train loss 30.8939, valid loss 31.4180\n","Epoch 0016: train loss 30.9409, valid loss 31.1315\n","Epoch 0017: train loss 31.0024, valid loss 30.7104\n","Epoch 0018: train loss 30.8801, valid loss 30.9773\n","Epoch 0019: train loss 30.8584, valid loss 31.0234\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:35:18,464] Trial 43 finished with value: 30.720640553963765 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00032426765948255243, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.4729, valid loss 30.7206\n","Finished Training.\n","Epoch 0001: train loss 88.3870, valid loss 42.8056\n","Epoch 0002: train loss 40.6660, valid loss 39.8217\n","Epoch 0003: train loss 39.4843, valid loss 38.6169\n","Epoch 0004: train loss 34.9416, valid loss 32.3491\n","Epoch 0005: train loss 31.5716, valid loss 31.7491\n","Epoch 0006: train loss 31.4957, valid loss 31.6779\n","Epoch 0007: train loss 31.3091, valid loss 31.8896\n","Epoch 0008: train loss 31.2355, valid loss 31.2073\n","Epoch 0009: train loss 31.1490, valid loss 30.9710\n","Epoch 0010: train loss 31.0563, valid loss 31.1506\n","Epoch 0011: train loss 30.9617, valid loss 31.3086\n","Epoch 0012: train loss 30.8013, valid loss 31.5270\n","Epoch 0013: train loss 30.8592, valid loss 31.1747\n","Epoch 0014: train loss 31.0385, valid loss 31.3217\n","Epoch 0015: train loss 30.8524, valid loss 30.8699\n","Epoch 0016: train loss 30.9103, valid loss 31.0333\n","Epoch 0017: train loss 30.9372, valid loss 31.4467\n","Epoch 0018: train loss 30.9707, valid loss 30.9363\n","Epoch 0019: train loss 30.8289, valid loss 31.2092\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:39:32,991] Trial 44 finished with value: 31.258531507927827 and parameters: {'hidden_dim': 64, 'latent_dim': 64, 'num_layers': 3, 'learning_rate': 0.00019820414424440375, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.7935, valid loss 31.2585\n","Finished Training.\n","Epoch 0001: train loss 71.9462, valid loss 35.1059\n","Epoch 0002: train loss 32.7444, valid loss 32.0542\n","Epoch 0003: train loss 31.7051, valid loss 31.9618\n","Epoch 0004: train loss 31.5076, valid loss 31.8443\n","Epoch 0005: train loss 31.1499, valid loss 31.3273\n","Epoch 0006: train loss 31.2304, valid loss 30.9393\n","Epoch 0007: train loss 31.0231, valid loss 32.8274\n","Epoch 0008: train loss 31.0650, valid loss 31.0457\n","Epoch 0009: train loss 31.0263, valid loss 30.8832\n","Epoch 0010: train loss 31.0392, valid loss 31.1055\n","Epoch 0011: train loss 31.1284, valid loss 30.9870\n","Epoch 0012: train loss 30.9122, valid loss 31.0290\n","Epoch 0013: train loss 30.8952, valid loss 30.8725\n","Epoch 0014: train loss 30.7880, valid loss 31.1312\n","Epoch 0015: train loss 30.7543, valid loss 31.0030\n","Epoch 0016: train loss 30.8252, valid loss 30.9398\n","Epoch 0017: train loss 30.8760, valid loss 30.8158\n","Epoch 0018: train loss 30.8308, valid loss 31.1149\n","Epoch 0019: train loss 30.8543, valid loss 31.0949\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:43:47,157] Trial 45 finished with value: 30.963114820616077 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00027565045050337116, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 31.0532, valid loss 30.9631\n","Finished Training.\n","Epoch 0001: train loss 54.4152, valid loss 32.2971\n","Epoch 0002: train loss 32.3491, valid loss 32.4930\n","Epoch 0003: train loss 31.6256, valid loss 31.8743\n","Epoch 0004: train loss 31.4820, valid loss 32.3577\n","Epoch 0005: train loss 31.3499, valid loss 31.5341\n","Epoch 0006: train loss 31.3617, valid loss 31.6298\n","Epoch 0007: train loss 31.1849, valid loss 31.1271\n","Epoch 0008: train loss 31.2194, valid loss 31.2605\n","Epoch 0009: train loss 31.1580, valid loss 31.1755\n","Epoch 0010: train loss 31.0904, valid loss 31.3692\n","Epoch 0011: train loss 31.1257, valid loss 31.1913\n","Epoch 0012: train loss 31.1017, valid loss 31.2949\n","Epoch 0013: train loss 30.9780, valid loss 31.0426\n","Epoch 0014: train loss 30.9305, valid loss 31.2175\n","Epoch 0015: train loss 30.9975, valid loss 30.8630\n","Epoch 0016: train loss 31.2770, valid loss 32.0697\n","Epoch 0017: train loss 30.8262, valid loss 31.5368\n","Epoch 0018: train loss 30.7856, valid loss 31.2730\n","Epoch 0019: train loss 30.8511, valid loss 31.1510\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:46:50,614] Trial 46 finished with value: 30.957893989505838 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 2, 'learning_rate': 0.0007320268346303999, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8025, valid loss 30.9579\n","Finished Training.\n","Epoch 0001: train loss 49.8892, valid loss 32.4662\n","Epoch 0002: train loss 32.0395, valid loss 32.3822\n","Epoch 0003: train loss 31.6866, valid loss 32.0126\n","Epoch 0004: train loss 31.5383, valid loss 31.0666\n","Epoch 0005: train loss 31.2361, valid loss 31.2618\n","Epoch 0006: train loss 31.3212, valid loss 31.4441\n","Epoch 0007: train loss 31.3039, valid loss 31.3344\n","Epoch 0008: train loss 31.1001, valid loss 31.2331\n","Epoch 0009: train loss 31.2700, valid loss 31.5241\n","Epoch 0010: train loss 31.0998, valid loss 33.0471\n","Epoch 0011: train loss 30.4065, valid loss 30.7006\n","Epoch 0012: train loss 30.4834, valid loss 30.6707\n","Epoch 0013: train loss 30.4465, valid loss 30.8753\n","Epoch 0014: train loss 30.5272, valid loss 30.7474\n","Epoch 0015: train loss 30.3304, valid loss 30.5824\n","Epoch 0016: train loss 30.3837, valid loss 31.0416\n","Epoch 0017: train loss 30.4245, valid loss 30.9284\n","Epoch 0018: train loss 30.4943, valid loss 30.9974\n","Epoch 0019: train loss 30.3391, valid loss 30.8348\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:51:04,194] Trial 47 finished with value: 30.840765792332338 and parameters: {'hidden_dim': 64, 'latent_dim': 16, 'num_layers': 3, 'learning_rate': 0.0009551720692705692, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.5077, valid loss 30.8408\n","Finished Training.\n","Epoch 0001: train loss 132.4852, valid loss 76.8057\n","Epoch 0002: train loss 66.2221, valid loss 70.4649\n","Epoch 0003: train loss 63.5639, valid loss 62.8397\n","Epoch 0004: train loss 62.8784, valid loss 64.8588\n","Epoch 0005: train loss 62.8502, valid loss 67.5388\n","Epoch 0006: train loss 62.5437, valid loss 61.8976\n","Epoch 0007: train loss 62.0948, valid loss 62.6066\n","Epoch 0008: train loss 62.0928, valid loss 62.6369\n","Epoch 0009: train loss 62.1902, valid loss 62.8973\n","Epoch 0010: train loss 62.2159, valid loss 64.6855\n","Epoch 0011: train loss 62.2694, valid loss 62.7934\n","Epoch 0012: train loss 62.0443, valid loss 62.7091\n","Epoch 0013: train loss 60.9741, valid loss 62.0512\n","Epoch 0014: train loss 60.8012, valid loss 61.3814\n","Epoch 0015: train loss 60.6681, valid loss 61.8465\n","Epoch 0016: train loss 60.5266, valid loss 61.5702\n","Epoch 0017: train loss 61.1593, valid loss 61.5576\n","Epoch 0018: train loss 60.8602, valid loss 61.8299\n","Epoch 0019: train loss 60.8477, valid loss 61.1140\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:55:45,246] Trial 48 finished with value: 62.19773724730988 and parameters: {'hidden_dim': 128, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00043809017319436437, 'batch_size': 32}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 60.8481, valid loss 62.1977\n","Finished Training.\n","Epoch 0001: train loss 114.9127, valid loss 44.6689\n","Epoch 0002: train loss 40.3617, valid loss 38.0207\n","Epoch 0003: train loss 35.3235, valid loss 32.6977\n","Epoch 0004: train loss 32.0587, valid loss 32.1668\n","Epoch 0005: train loss 31.6050, valid loss 32.2038\n","Epoch 0006: train loss 31.4338, valid loss 31.3723\n","Epoch 0007: train loss 31.3153, valid loss 31.1373\n","Epoch 0008: train loss 31.0762, valid loss 31.4471\n","Epoch 0009: train loss 30.8554, valid loss 31.3886\n","Epoch 0010: train loss 30.9475, valid loss 31.2266\n","Epoch 0011: train loss 30.9536, valid loss 31.2991\n","Epoch 0012: train loss 30.8904, valid loss 31.0253\n","Epoch 0013: train loss 30.8637, valid loss 31.1443\n","Epoch 0014: train loss 30.8948, valid loss 31.9729\n","Epoch 0015: train loss 30.8221, valid loss 31.1542\n","Epoch 0016: train loss 30.7399, valid loss 30.8739\n","Epoch 0017: train loss 30.6709, valid loss 31.1048\n","Epoch 0018: train loss 30.9284, valid loss 31.3669\n","Epoch 0019: train loss 30.8145, valid loss 31.3261\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-10-05 18:59:58,238] Trial 49 finished with value: 30.97142788176233 and parameters: {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.00013959292792355463, 'batch_size': 16}. Best is trial 34 with value: 30.610877956790425.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0020: train loss 30.8193, valid loss 30.9714\n","Finished Training.\n","Best hyperparameters:  {'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0002398370047454381, 'batch_size': 16}\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","joblib.dump(study, \"study.pkl\")   # save study\n","\n","# to load it:\n","jl = joblib.load(\"study.pkl\")\n","\n","print(jl.best_trial.params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnJg0VKzcUQF","executionInfo":{"status":"ok","timestamp":1759685769029,"user_tz":-120,"elapsed":4,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"c5cdfeb3-1a95-4dc6-dc92-68e05db55961"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["{'hidden_dim': 64, 'latent_dim': 32, 'num_layers': 3, 'learning_rate': 0.0002398370047454381, 'batch_size': 16}\n"]}]},{"cell_type":"markdown","source":["# Evaluate"],"metadata":{"id":"hBvqbBDn7-Lk"}},{"cell_type":"code","source":["def evaluate_lstm(model, test_loader, device, percentile_threshold=90):\n","    model.eval()\n","    anomaly_scores = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","\n","            batch_scores = []\n","            for i in range(batch.shape[0]): #Iterate through each sequence in the batch\n","                sequence = batch[i, :, :].unsqueeze(0)  # Select a single sequence\n","                recon_batch, mean, logvar = model(sequence)\n","                loss = loss_function(recon_batch, sequence, mean, logvar)\n","                batch_scores.append(loss.item())\n","            anomaly_scores.extend(batch_scores)  # Append scores for all sequences in the batch\n","\n","\n","    # Calculate the threshold based on the specified percentile\n","    threshold = np.percentile(anomaly_scores, percentile_threshold)\n","\n","    # Identify anomaly indices\n","    anomaly_indices = [i for i, score in enumerate(anomaly_scores) if score > threshold]\n","    return anomaly_indices\n","anomalies = evaluate_lstm(model, test_loader, device, 90)"],"metadata":{"id":"Tm2z5saXKo1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_f1_score(anomaly_indices, true_anomalies):\n","    # Create a binary array representing predicted anomalies\n","    predicted_anomalies = np.zeros_like(true_anomalies)\n","    for index in anomaly_indices:\n","        if index < len(predicted_anomalies):  # Check index bounds\n","          predicted_anomalies[index] = 1\n","\n","    # Calculate the F1 score\n","    f1 = f1_score(true_anomalies, predicted_anomalies)\n","    return f1, predicted_anomalies\n","\n","# Example usage (assuming 'anomalies' and 'true_anomalies' are defined)\n","f1, predicted_anomalies = calculate_f1_score(anomalies, true_anomalies)\n","print(f\"F1 Score: {f1}\")"],"metadata":{"id":"KqE9hY30I1a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(true_anomalies, predicted_anomalies))"],"metadata":{"id":"2gnQeqVxZ5HY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(confusion_matrix(true_anomalies, predicted_anomalies))"],"metadata":{"id":"f9ymbpd2amAr"},"execution_count":null,"outputs":[]}]}