{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1sQnANEkSah2RJJnJkDtigWih8Z0QGGNC","authorship_tag":"ABX9TyOMbhNODeRo4f+erbq+y8R1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup Environment and Read Data"],"metadata":{"id":"YPOOHjNpYkOQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRI6Ne_LYfaB"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import copy\n","from tqdm import trange,tqdm\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import f1_score, classification_report, confusion_matrix"]},{"cell_type":"markdown","source":["## Setup the Dataset"],"metadata":{"id":"y2w0Pp7uYrCb"}},{"cell_type":"code","source":["DRIVE = \"drive/MyDrive/Colab Notebooks/ELTE/DSLAB/ServerMachineDataset/\"\n","MACHINE = \"machine-1-1.txt\"\n","TRAIN_DATASET = DRIVE + \"train/\" + MACHINE\n","TEST_DATASET = DRIVE + \"test/\" + MACHINE\n","TEST_LABEL_DATASET = DRIVE + \"test_label/\" + MACHINE\n","\n","metric = pd.read_csv(TRAIN_DATASET, header=None)\n","metric_test = pd.read_csv(TEST_DATASET, header=None)\n","true_anomalies = pd.read_csv(TEST_LABEL_DATASET, header=None)[0].to_numpy()"],"metadata":{"id":"WmV09uWKYsJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric"],"metadata":{"id":"B90bIlxWYtiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746389609337,"user_tz":-120,"elapsed":113,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"b7e55616-d73b-4a3a-e053-d411b4add3d7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             0         1         2         3    4         5         6    7   \\\n","0      0.032258  0.039195  0.027871  0.024390  0.0  0.915385  0.343691  0.0   \n","1      0.043011  0.048729  0.033445  0.025552  0.0  0.915385  0.344633  0.0   \n","2      0.043011  0.034958  0.032330  0.025552  0.0  0.915385  0.344633  0.0   \n","3      0.032258  0.028602  0.030100  0.024390  0.0  0.912821  0.342750  0.0   \n","4      0.032258  0.019068  0.026756  0.023229  0.0  0.912821  0.342750  0.0   \n","...         ...       ...       ...       ...  ...       ...       ...  ...   \n","28474  0.075269  0.046610  0.071349  0.076655  0.0  0.928205  0.269303  0.0   \n","28475  0.086022  0.070975  0.075808  0.077816  0.0  0.930769  0.269303  0.0   \n","28476  0.086022  0.065678  0.073579  0.076655  0.0  0.935897  0.270245  0.0   \n","28477  0.086022  0.056144  0.068004  0.074332  0.0  0.933333  0.271186  0.0   \n","28478  0.075269  0.081568  0.072464  0.075494  0.0  0.933333  0.273070  0.0   \n","\n","             8         9   ...   28        29        30        31        32  \\\n","0      0.020011  0.000122  ...  0.0  0.004298  0.029993  0.022131  0.000000   \n","1      0.019160  0.001722  ...  0.0  0.004298  0.030041  0.028821  0.000000   \n","2      0.020011  0.000122  ...  0.0  0.004298  0.026248  0.021101  0.000000   \n","3      0.021289  0.000000  ...  0.0  0.004298  0.030169  0.025733  0.000000   \n","4      0.018734  0.000000  ...  0.0  0.004298  0.027240  0.022645  0.000000   \n","...         ...       ...  ...  ...       ...       ...       ...       ...   \n","28474  0.031649  0.000244  ...  0.0  0.008596  0.068980  0.049408  0.000386   \n","28475  0.029946  0.000244  ...  0.0  0.008596  0.073029  0.055584  0.000386   \n","28476  0.030372  0.000244  ...  0.0  0.008596  0.070516  0.048893  0.000386   \n","28477  0.032643  0.000244  ...  0.0  0.008596  0.070308  0.055069  0.000386   \n","28478  0.031791  0.000000  ...  0.0  0.008596  0.067924  0.048893  0.000386   \n","\n","             33        34        35   36   37  \n","0      0.000045  0.034677  0.034747  0.0  0.0  \n","1      0.000045  0.035763  0.035833  0.0  0.0  \n","2      0.000045  0.033012  0.033082  0.0  0.0  \n","3      0.000022  0.035112  0.035182  0.0  0.0  \n","4      0.000034  0.033447  0.033517  0.0  0.0  \n","...         ...       ...       ...  ...  ...  \n","28474  0.000034  0.064504  0.064572  0.0  0.0  \n","28475  0.000034  0.067690  0.067757  0.0  0.0  \n","28476  0.000034  0.064866  0.064934  0.0  0.0  \n","28477  0.000045  0.067111  0.067178  0.0  0.0  \n","28478  0.000022  0.065011  0.065079  0.0  0.0  \n","\n","[28479 rows x 38 columns]"],"text/html":["\n","  <div id=\"df-bf5641b0-63c2-4748-b3aa-029e9bffd8e8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.032258</td>\n","      <td>0.039195</td>\n","      <td>0.027871</td>\n","      <td>0.024390</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.343691</td>\n","      <td>0.0</td>\n","      <td>0.020011</td>\n","      <td>0.000122</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.029993</td>\n","      <td>0.022131</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.034677</td>\n","      <td>0.034747</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.043011</td>\n","      <td>0.048729</td>\n","      <td>0.033445</td>\n","      <td>0.025552</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.344633</td>\n","      <td>0.0</td>\n","      <td>0.019160</td>\n","      <td>0.001722</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.030041</td>\n","      <td>0.028821</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.035763</td>\n","      <td>0.035833</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.043011</td>\n","      <td>0.034958</td>\n","      <td>0.032330</td>\n","      <td>0.025552</td>\n","      <td>0.0</td>\n","      <td>0.915385</td>\n","      <td>0.344633</td>\n","      <td>0.0</td>\n","      <td>0.020011</td>\n","      <td>0.000122</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.026248</td>\n","      <td>0.021101</td>\n","      <td>0.000000</td>\n","      <td>0.000045</td>\n","      <td>0.033012</td>\n","      <td>0.033082</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.032258</td>\n","      <td>0.028602</td>\n","      <td>0.030100</td>\n","      <td>0.024390</td>\n","      <td>0.0</td>\n","      <td>0.912821</td>\n","      <td>0.342750</td>\n","      <td>0.0</td>\n","      <td>0.021289</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.030169</td>\n","      <td>0.025733</td>\n","      <td>0.000000</td>\n","      <td>0.000022</td>\n","      <td>0.035112</td>\n","      <td>0.035182</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.032258</td>\n","      <td>0.019068</td>\n","      <td>0.026756</td>\n","      <td>0.023229</td>\n","      <td>0.0</td>\n","      <td>0.912821</td>\n","      <td>0.342750</td>\n","      <td>0.0</td>\n","      <td>0.018734</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.004298</td>\n","      <td>0.027240</td>\n","      <td>0.022645</td>\n","      <td>0.000000</td>\n","      <td>0.000034</td>\n","      <td>0.033447</td>\n","      <td>0.033517</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>28474</th>\n","      <td>0.075269</td>\n","      <td>0.046610</td>\n","      <td>0.071349</td>\n","      <td>0.076655</td>\n","      <td>0.0</td>\n","      <td>0.928205</td>\n","      <td>0.269303</td>\n","      <td>0.0</td>\n","      <td>0.031649</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.068980</td>\n","      <td>0.049408</td>\n","      <td>0.000386</td>\n","      <td>0.000034</td>\n","      <td>0.064504</td>\n","      <td>0.064572</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28475</th>\n","      <td>0.086022</td>\n","      <td>0.070975</td>\n","      <td>0.075808</td>\n","      <td>0.077816</td>\n","      <td>0.0</td>\n","      <td>0.930769</td>\n","      <td>0.269303</td>\n","      <td>0.0</td>\n","      <td>0.029946</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.073029</td>\n","      <td>0.055584</td>\n","      <td>0.000386</td>\n","      <td>0.000034</td>\n","      <td>0.067690</td>\n","      <td>0.067757</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28476</th>\n","      <td>0.086022</td>\n","      <td>0.065678</td>\n","      <td>0.073579</td>\n","      <td>0.076655</td>\n","      <td>0.0</td>\n","      <td>0.935897</td>\n","      <td>0.270245</td>\n","      <td>0.0</td>\n","      <td>0.030372</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.070516</td>\n","      <td>0.048893</td>\n","      <td>0.000386</td>\n","      <td>0.000034</td>\n","      <td>0.064866</td>\n","      <td>0.064934</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28477</th>\n","      <td>0.086022</td>\n","      <td>0.056144</td>\n","      <td>0.068004</td>\n","      <td>0.074332</td>\n","      <td>0.0</td>\n","      <td>0.933333</td>\n","      <td>0.271186</td>\n","      <td>0.0</td>\n","      <td>0.032643</td>\n","      <td>0.000244</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.070308</td>\n","      <td>0.055069</td>\n","      <td>0.000386</td>\n","      <td>0.000045</td>\n","      <td>0.067111</td>\n","      <td>0.067178</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>28478</th>\n","      <td>0.075269</td>\n","      <td>0.081568</td>\n","      <td>0.072464</td>\n","      <td>0.075494</td>\n","      <td>0.0</td>\n","      <td>0.933333</td>\n","      <td>0.273070</td>\n","      <td>0.0</td>\n","      <td>0.031791</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.008596</td>\n","      <td>0.067924</td>\n","      <td>0.048893</td>\n","      <td>0.000386</td>\n","      <td>0.000022</td>\n","      <td>0.065011</td>\n","      <td>0.065079</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28479 rows × 38 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf5641b0-63c2-4748-b3aa-029e9bffd8e8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bf5641b0-63c2-4748-b3aa-029e9bffd8e8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bf5641b0-63c2-4748-b3aa-029e9bffd8e8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-eb48af23-49b9-4476-9b5a-2b7ceab18dd9\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb48af23-49b9-4476-9b5a-2b7ceab18dd9')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-eb48af23-49b9-4476-9b5a-2b7ceab18dd9 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_0a4a0e58-d7e6-455c-88fe-60fbf08826a8\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metric')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_0a4a0e58-d7e6-455c-88fe-60fbf08826a8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('metric');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"metric"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Preprocess the Dataset"],"metadata":{"id":"gKiXMEKYYxC-"}},{"cell_type":"markdown","source":["### Non-scaled"],"metadata":{"id":"YweMW2-7Yy0F"}},{"cell_type":"code","source":["# create train and test dataloaders\n","metric.interpolate(inplace=True)\n","metric.bfill(inplace=True)\n","metric_tensor = metric.values\n","\n","metric_test.interpolate(inplace=True)\n","metric_test.bfill(inplace=True)\n","metric_test_tensor = metric_test.values\n","\n","sequence_length = 30\n","sequences = []\n","for i in range(metric_tensor.shape[0] - sequence_length + 1):\n","  sequences.append(metric_tensor[i:i + sequence_length])\n","\n","train_data, val_data = train_test_split(sequences, test_size=0.3, random_state=42) # 70% train, 30% temp\n","\n","test_sequences = []\n","for i in range(metric_test_tensor.shape[0] - sequence_length + 1):\n","  test_sequences.append(metric_test_tensor[i:i + sequence_length])\n","\n","batch_size = 32\n","train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(dataset=test_sequences, batch_size=batch_size, shuffle=False)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"NgLJxu45Yz9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences[0].shape"],"metadata":{"id":"-yjnxNiOY1nJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746389609442,"user_tz":-120,"elapsed":55,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"47c62deb-a356-4b6b-eef1-8ea74fe37fa0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30, 38)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Define the Networks"],"metadata":{"id":"db2NTe4nY4-h"}},{"cell_type":"code","source":["input_dim = 38\n","hidden_dim = 128\n","latent_dim = 32"],"metadata":{"id":"QWBtLBUmw0rL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_model(model_name, model, input_dim, latent_dim, hidden_dim):\n","    model_state = {\n","        'input_dim':input_dim,\n","        'latent_dim':latent_dim,\n","        'hidden_dim':hidden_dim,\n","        'state_dict':model.state_dict()\n","    }\n","    torch.save(model_state, f'drive/MyDrive/Colab Notebooks/ELTE/DSLAB/{model_name}.pth')"],"metadata":{"id":"RHrbCKugwvVS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AutoEncoder"],"metadata":{"id":"AQ-8xmXtY7af"}},{"cell_type":"code","source":["class FeedforwardEncoder(nn.Module):\n","    def __init__(self, input_dim, sequence_length, hidden_dim, latent_dim):\n","        super(FeedforwardEncoder, self).__init__()\n","        self.flatten_dim = input_dim * sequence_length\n","        self.encoder = nn.Sequential(\n","            nn.Linear(self.flatten_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, latent_dim)\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten: (batch_size, sequence_length * input_dim)\n","        z = self.encoder(x)\n","        return z\n","\n","class FeedforwardDecoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim, sequence_length):\n","        super(FeedforwardDecoder, self).__init__()\n","        self.output_dim = output_dim\n","        self.sequence_length = sequence_length\n","        self.decoder = nn.Sequential(\n","            nn.Linear(latent_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim * sequence_length)\n","        )\n","\n","    def forward(self, z):\n","        x_recon = self.decoder(z)\n","        return x_recon.view(z.size(0), self.sequence_length, self.output_dim)\n","\n","class AE(nn.Module):\n","    def __init__(self, input_dim, sequence_length, hidden_dim, latent_dim, device='cpu'):\n","        super(AE, self).__init__()\n","        self.encoder = FeedforwardEncoder(input_dim, sequence_length, hidden_dim, latent_dim).to(device)\n","        self.decoder = FeedforwardDecoder(latent_dim, hidden_dim, input_dim, sequence_length).to(device)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_recon = self.decoder(z)\n","        return x_recon"],"metadata":{"id":"rlMMyXqlY-OP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_function_ae(x, x_hat):\n","    return nn.functional.mse_loss(x_hat, x, reduction='sum')"],"metadata":{"id":"HXmz9M74wqCf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_ae = AE(input_dim=input_dim,\n","                hidden_dim=hidden_dim,\n","                latent_dim=latent_dim,\n","                sequence_length=sequence_length,\n","                device=device).to(device)\n","optimizer_ae = Adam(model_ae.parameters(), lr=1e-3)\n","scheduler_ae = ReduceLROnPlateau(optimizer_ae, 'min', patience=5, factor=0.1, verbose=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZT7Zk-3whgH","executionInfo":{"status":"ok","timestamp":1746389616254,"user_tz":-120,"elapsed":6786,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"5651bad6-17a3-4216-ebe3-c7b423865f23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## LSTM AutoEncoder"],"metadata":{"id":"hLDbcntSxFKF"}},{"cell_type":"code","source":["class LSTMEncoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n","        super(LSTMEncoder, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x):\n","        _, (h_n, _) = self.lstm(x)\n","        h = h_n[-1]  # Take last layer's hidden state\n","        z = self.fc(h)\n","        return z\n","\n","class LSTMDecoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim, sequence_length, num_layers=1):\n","        super(LSTMDecoder, self).__init__()\n","        self.sequence_length = sequence_length\n","        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.output_layer = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, z):\n","        hidden = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)\n","        out, _ = self.lstm(hidden)\n","        return self.output_layer(out)\n","\n","\n","class LSTMAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, sequence_length, num_layers=1, device='cpu'):\n","        super(LSTMAE, self).__init__()\n","        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim, num_layers).to(device)\n","        self.decoder = LSTMDecoder(latent_dim, hidden_dim, input_dim, sequence_length, num_layers).to(device)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        x_recon = self.decoder(z)\n","        return x_recon"],"metadata":{"id":"mC2FTiXM22xf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_function_lstm_ae(x, x_hat):\n","    return nn.functional.mse_loss(x_hat, x, reduction='sum')"],"metadata":{"id":"w3eGQEV422za"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_lstm_ae = LSTMAE(input_dim=input_dim,\n","                hidden_dim=hidden_dim,\n","                latent_dim=latent_dim,\n","                sequence_length=sequence_length,\n","                num_layers=1,\n","                device=device).to(device)\n","optimizer_lstm_ae = Adam(model_lstm_ae.parameters(), lr=1e-3)\n","scheduler_lstm_ae = ReduceLROnPlateau(optimizer_lstm_ae, 'min', patience=5, factor=0.1, verbose=True)"],"metadata":{"id":"N2PkeD3e221c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM Variational AutoEncoder"],"metadata":{"id":"EBwVWm5zxKGz"}},{"cell_type":"code","source":["class LSTMEncoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):\n","        super(LSTMEncoder, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n","        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x):\n","        _, (h_n, _) = self.lstm(x)  # h_n: (num_layers, batch, hidden_dim)\n","        h = h_n[-1]  # take the output of the last layer\n","        return self.fc_mean(h), self.fc_logvar(h)\n","\n","\n","class LSTMDecoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim, sequence_length, num_layers=1):\n","        super(LSTMDecoder, self).__init__()\n","        self.sequence_length = sequence_length\n","        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.output_layer = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, z):\n","        # Repeat z for each timestep\n","        hidden = self.latent_to_hidden(z).unsqueeze(1).repeat(1, self.sequence_length, 1)\n","        out, _ = self.lstm(hidden)\n","        return self.output_layer(out)\n","\n","\n","class LSTMVAE(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim, sequence_length, num_layers=1, device='cpu'):\n","        super(LSTMVAE, self).__init__()\n","        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim, num_layers).to(device)\n","        self.decoder = LSTMDecoder(latent_dim, hidden_dim, input_dim, sequence_length, num_layers).to(device)\n","\n","    def reparameterize(self, mean, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mean + eps * std\n","\n","    def forward(self, x):\n","        mean, logvar = self.encoder(x)\n","        z = self.reparameterize(mean, logvar)\n","        x_recon = self.decoder(z)\n","        return x_recon, mean, logvar"],"metadata":{"id":"rk0prcNW3D2K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_function_lstm_vae(x, x_hat, mean, log_var):\n","    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n","    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n","\n","    return reproduction_loss + KLD"],"metadata":{"id":"mhXR-zd43D4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_lstm_vae = LSTMVAE(input_dim=input_dim,\n","                hidden_dim=hidden_dim,\n","                latent_dim=latent_dim,\n","                sequence_length=sequence_length,\n","                num_layers=1,\n","                device=device).to(device)\n","optimizer_lstm_vae = Adam(model_lstm_vae.parameters(), lr=1e-3)\n","scheduler_lstm_vae = ReduceLROnPlateau(optimizer_lstm_vae, 'min', patience=5, factor=0.1, verbose=True)"],"metadata":{"id":"PhiV3B9z3EHW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"pxHd5ffExMmv"}},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, optimizer, loss_fn, scheduler, variational=False, num_epochs=10, device='cpu'):\n","    torch.cuda.empty_cache()\n","    train_losses = []\n","    val_losses = []\n","\n","    early_stop_tolerant_count = 0\n","    early_stop_tolerant = 10\n","    best_loss = float('inf')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    for epoch in range(num_epochs):\n","        train_loss = 0.0\n","        model.train()\n","        for batch in train_loader:\n","            batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","\n","            optimizer.zero_grad()\n","\n","            if variational:\n","                recon_batch, mean, logvar = model(batch)\n","                loss = loss_fn(recon_batch, batch, mean, logvar)\n","            else:\n","                recon_batch = model(batch)\n","                loss = loss_fn(batch, recon_batch)\n","\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_loss /= len(train_loader)\n","        train_losses.append(train_loss)\n","\n","        # Validation\n","        model.eval()\n","        valid_loss = 0.0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","                if variational:\n","                    recon_batch, mean, logvar = model(batch)\n","                    loss = loss_fn(recon_batch, batch, mean, logvar)\n","                else:\n","                    recon_batch = model(batch)\n","                    loss = loss_fn(batch, recon_batch)\n","                valid_loss += loss.item()\n","\n","        valid_loss /= len(val_loader)\n","        val_losses.append(valid_loss)\n","\n","        scheduler.step(valid_loss)\n","\n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            early_stop_tolerant_count = 0\n","        else:\n","            early_stop_tolerant_count += 1\n","\n","        print(f\"Epoch {epoch+1:04d}: train loss {train_loss:.4f}, valid loss {valid_loss:.4f}\")\n","\n","        if early_stop_tolerant_count >= early_stop_tolerant:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    model.load_state_dict(best_model_wts)\n","    print(\"Finished Training.\")\n","    return train_losses, val_losses"],"metadata":{"id":"1iMGHmHZx9Ok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AutoEncoder"],"metadata":{"id":"lL14o4MGx6yg"}},{"cell_type":"code","source":["train_losses_ae, val_losses_ae = train_model(model_ae, train_loader, val_loader, optimizer_ae, loss_function_ae, scheduler_ae, False, num_epochs=100, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soZM7l65yLLT","executionInfo":{"status":"ok","timestamp":1746389832190,"user_tz":-120,"elapsed":176318,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"ebd1a943-fc0e-4756-ec7f-0155ea4d227e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-eb483228514d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","<ipython-input-17-eb483228514d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0001: train loss 53.8265, valid loss 9.6677\n","Epoch 0002: train loss 8.5551, valid loss 8.4994\n","Epoch 0003: train loss 7.6650, valid loss 7.5286\n","Epoch 0004: train loss 6.6135, valid loss 6.7522\n","Epoch 0005: train loss 6.3623, valid loss 6.6303\n","Epoch 0006: train loss 6.2333, valid loss 6.5250\n","Epoch 0007: train loss 6.0628, valid loss 6.3578\n","Epoch 0008: train loss 5.8341, valid loss 6.2207\n","Epoch 0009: train loss 5.6077, valid loss 5.8243\n","Epoch 0010: train loss 5.3126, valid loss 5.5867\n","Epoch 0011: train loss 5.0831, valid loss 5.3053\n","Epoch 0012: train loss 4.8848, valid loss 5.1182\n","Epoch 0013: train loss 4.7216, valid loss 4.8789\n","Epoch 0014: train loss 4.5616, valid loss 4.7971\n","Epoch 0015: train loss 4.4782, valid loss 4.6007\n","Epoch 0016: train loss 4.3787, valid loss 4.4958\n","Epoch 0017: train loss 4.2963, valid loss 4.4564\n","Epoch 0018: train loss 4.2312, valid loss 4.3950\n","Epoch 0019: train loss 4.1390, valid loss 4.3058\n","Epoch 0020: train loss 4.0658, valid loss 4.2620\n","Epoch 0021: train loss 4.0285, valid loss 4.6962\n","Epoch 0022: train loss 4.0138, valid loss 4.4360\n","Epoch 0023: train loss 3.9407, valid loss 4.1075\n","Epoch 0024: train loss 3.9180, valid loss 4.1552\n","Epoch 0025: train loss 3.8516, valid loss 4.1436\n","Epoch 0026: train loss 3.8100, valid loss 4.0052\n","Epoch 0027: train loss 3.7934, valid loss 4.0375\n","Epoch 0028: train loss 3.7568, valid loss 4.1945\n","Epoch 0029: train loss 3.7449, valid loss 4.2604\n","Epoch 0030: train loss 3.7288, valid loss 4.3134\n","Epoch 0031: train loss 3.7183, valid loss 3.9927\n","Epoch 0032: train loss 3.6792, valid loss 3.8532\n","Epoch 0033: train loss 3.6824, valid loss 3.9350\n","Epoch 0034: train loss 3.6697, valid loss 3.9607\n","Epoch 0035: train loss 3.6436, valid loss 3.7792\n","Epoch 0036: train loss 3.6597, valid loss 3.7002\n","Epoch 0037: train loss 3.6213, valid loss 3.7660\n","Epoch 0038: train loss 3.6180, valid loss 3.8557\n","Epoch 0039: train loss 3.6047, valid loss 3.7994\n","Epoch 0040: train loss 3.6097, valid loss 3.8421\n","Epoch 0041: train loss 3.5836, valid loss 3.6810\n","Epoch 0042: train loss 3.5752, valid loss 3.7818\n","Epoch 0043: train loss 3.5912, valid loss 3.6769\n","Epoch 0044: train loss 3.5647, valid loss 3.7615\n","Epoch 0045: train loss 3.5570, valid loss 3.7647\n","Epoch 0046: train loss 3.5658, valid loss 3.6732\n","Epoch 0047: train loss 3.5449, valid loss 3.7239\n","Epoch 0048: train loss 3.5362, valid loss 3.6850\n","Epoch 0049: train loss 3.5094, valid loss 3.7263\n","Epoch 0050: train loss 3.5276, valid loss 3.6463\n","Epoch 0051: train loss 3.5042, valid loss 3.6636\n","Epoch 0052: train loss 3.4819, valid loss 3.6259\n","Epoch 0053: train loss 3.4851, valid loss 3.6677\n","Epoch 0054: train loss 3.4763, valid loss 3.6480\n","Epoch 0055: train loss 3.4489, valid loss 3.5782\n","Epoch 0056: train loss 3.4513, valid loss 3.6751\n","Epoch 0057: train loss 3.4233, valid loss 3.5468\n","Epoch 0058: train loss 3.4169, valid loss 3.5948\n","Epoch 0059: train loss 3.3935, valid loss 3.7684\n","Epoch 0060: train loss 3.4028, valid loss 3.5345\n","Epoch 0061: train loss 3.3872, valid loss 3.6289\n","Epoch 0062: train loss 3.3805, valid loss 3.5676\n","Epoch 0063: train loss 3.3680, valid loss 3.6078\n","Epoch 0064: train loss 3.3639, valid loss 3.5696\n","Epoch 0065: train loss 3.3579, valid loss 3.5344\n","Epoch 0066: train loss 3.3535, valid loss 3.6148\n","Epoch 0067: train loss 3.1382, valid loss 3.2522\n","Epoch 0068: train loss 3.1262, valid loss 3.2505\n","Epoch 0069: train loss 3.1245, valid loss 3.2582\n","Epoch 0070: train loss 3.1244, valid loss 3.2538\n","Epoch 0071: train loss 3.1233, valid loss 3.2465\n","Epoch 0072: train loss 3.1211, valid loss 3.2532\n","Epoch 0073: train loss 3.1200, valid loss 3.2545\n","Epoch 0074: train loss 3.1179, valid loss 3.2388\n","Epoch 0075: train loss 3.1159, valid loss 3.2466\n","Epoch 0076: train loss 3.1132, valid loss 3.2393\n","Epoch 0077: train loss 3.1116, valid loss 3.2414\n","Epoch 0078: train loss 3.1102, valid loss 3.2376\n","Epoch 0079: train loss 3.1090, valid loss 3.2348\n","Epoch 0080: train loss 3.1063, valid loss 3.2315\n","Epoch 0081: train loss 3.1048, valid loss 3.2383\n","Epoch 0082: train loss 3.1031, valid loss 3.2258\n","Epoch 0083: train loss 3.1008, valid loss 3.2283\n","Epoch 0084: train loss 3.0995, valid loss 3.2265\n","Epoch 0085: train loss 3.0979, valid loss 3.2252\n","Epoch 0086: train loss 3.0963, valid loss 3.2152\n","Epoch 0087: train loss 3.0940, valid loss 3.2180\n","Epoch 0088: train loss 3.0938, valid loss 3.2183\n","Epoch 0089: train loss 3.0907, valid loss 3.2149\n","Epoch 0090: train loss 3.0895, valid loss 3.2061\n","Epoch 0091: train loss 3.0880, valid loss 3.2172\n","Epoch 0092: train loss 3.0863, valid loss 3.2113\n","Epoch 0093: train loss 3.0843, valid loss 3.2110\n","Epoch 0094: train loss 3.0833, valid loss 3.2128\n","Epoch 0095: train loss 3.0808, valid loss 3.2042\n","Epoch 0096: train loss 3.0801, valid loss 3.2073\n","Epoch 0097: train loss 3.0774, valid loss 3.1964\n","Epoch 0098: train loss 3.0756, valid loss 3.1987\n","Epoch 0099: train loss 3.0746, valid loss 3.1940\n","Epoch 0100: train loss 3.0721, valid loss 3.2082\n","Finished Training.\n"]}]},{"cell_type":"markdown","source":["## LSTM AutoEncoder"],"metadata":{"id":"wXl7pCG1yQl1"}},{"cell_type":"code","source":["train_losses_lstm_ae, val_losses_lstm_ae = train_model(model_lstm_ae, train_loader, val_loader, optimizer_lstm_ae, loss_function_lstm_ae, scheduler_lstm_ae, False, num_epochs=100, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmC0XTG-z6Ja","executionInfo":{"status":"ok","timestamp":1746390114213,"user_tz":-120,"elapsed":282020,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"01936efe-70b6-4b85-a42d-eee7cfcf3c3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-eb483228514d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","<ipython-input-17-eb483228514d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0001: train loss 39.3473, valid loss 14.0262\n","Epoch 0002: train loss 10.7073, valid loss 9.7964\n","Epoch 0003: train loss 8.8445, valid loss 9.2932\n","Epoch 0004: train loss 8.4269, valid loss 8.6735\n","Epoch 0005: train loss 7.8551, valid loss 8.0608\n","Epoch 0006: train loss 7.5710, valid loss 7.9039\n","Epoch 0007: train loss 7.4262, valid loss 7.8936\n","Epoch 0008: train loss 7.3886, valid loss 7.8245\n","Epoch 0009: train loss 7.4033, valid loss 7.7002\n","Epoch 0010: train loss 7.3043, valid loss 7.5911\n","Epoch 0011: train loss 7.1531, valid loss 7.6514\n","Epoch 0012: train loss 6.4770, valid loss 6.8016\n","Epoch 0013: train loss 6.3444, valid loss 6.5142\n","Epoch 0014: train loss 6.2995, valid loss 6.3004\n","Epoch 0015: train loss 6.2720, valid loss 6.2892\n","Epoch 0016: train loss 6.1940, valid loss 6.4522\n","Epoch 0017: train loss 6.1870, valid loss 6.2083\n","Epoch 0018: train loss 6.1001, valid loss 6.3646\n","Epoch 0019: train loss 6.0902, valid loss 6.0575\n","Epoch 0020: train loss 6.0218, valid loss 6.3165\n","Epoch 0021: train loss 5.9439, valid loss 5.8747\n","Epoch 0022: train loss 5.7998, valid loss 5.9039\n","Epoch 0023: train loss 5.6380, valid loss 6.1006\n","Epoch 0024: train loss 5.7816, valid loss 5.9089\n","Epoch 0025: train loss 5.6014, valid loss 5.8416\n","Epoch 0026: train loss 5.5540, valid loss 5.6749\n","Epoch 0027: train loss 5.5886, valid loss 5.6886\n","Epoch 0028: train loss 5.5069, valid loss 5.6682\n","Epoch 0029: train loss 5.4613, valid loss 5.8001\n","Epoch 0030: train loss 5.4575, valid loss 5.5698\n","Epoch 0031: train loss 5.4605, valid loss 5.6510\n","Epoch 0032: train loss 5.4353, valid loss 5.4733\n","Epoch 0033: train loss 5.4157, valid loss 5.4707\n","Epoch 0034: train loss 5.3488, valid loss 5.3493\n","Epoch 0035: train loss 5.3657, valid loss 5.2698\n","Epoch 0036: train loss 5.3795, valid loss 5.4859\n","Epoch 0037: train loss 5.2924, valid loss 5.3069\n","Epoch 0038: train loss 5.2856, valid loss 5.2831\n","Epoch 0039: train loss 5.2633, valid loss 5.1841\n","Epoch 0040: train loss 5.2695, valid loss 5.1841\n","Epoch 0041: train loss 5.2297, valid loss 5.3003\n","Epoch 0042: train loss 5.2204, valid loss 5.7419\n","Epoch 0043: train loss 5.2189, valid loss 5.3117\n","Epoch 0044: train loss 5.1864, valid loss 5.2383\n","Epoch 0045: train loss 5.1653, valid loss 5.2630\n","Epoch 0046: train loss 4.8626, valid loss 4.8368\n","Epoch 0047: train loss 4.8019, valid loss 4.8105\n","Epoch 0048: train loss 4.7817, valid loss 4.8300\n","Epoch 0049: train loss 4.7742, valid loss 4.7743\n","Epoch 0050: train loss 4.7597, valid loss 4.7652\n","Epoch 0051: train loss 4.7469, valid loss 4.7784\n","Epoch 0052: train loss 4.7399, valid loss 4.7392\n","Epoch 0053: train loss 4.7267, valid loss 4.7323\n","Epoch 0054: train loss 4.7175, valid loss 4.7366\n","Epoch 0055: train loss 4.7094, valid loss 4.7367\n","Epoch 0056: train loss 4.7047, valid loss 4.6989\n","Epoch 0057: train loss 4.6943, valid loss 4.7042\n","Epoch 0058: train loss 4.6863, valid loss 4.7043\n","Epoch 0059: train loss 4.6814, valid loss 4.7477\n","Epoch 0060: train loss 4.6804, valid loss 4.6808\n","Epoch 0061: train loss 4.6654, valid loss 4.6724\n","Epoch 0062: train loss 4.6601, valid loss 4.6933\n","Epoch 0063: train loss 4.6644, valid loss 4.6631\n","Epoch 0064: train loss 4.6462, valid loss 4.6916\n","Epoch 0065: train loss 4.6472, valid loss 4.6755\n","Epoch 0066: train loss 4.6416, valid loss 4.6748\n","Epoch 0067: train loss 4.6417, valid loss 4.6390\n","Epoch 0068: train loss 4.6333, valid loss 4.6499\n","Epoch 0069: train loss 4.6303, valid loss 4.6327\n","Epoch 0070: train loss 4.6251, valid loss 4.6303\n","Epoch 0071: train loss 4.6209, valid loss 4.6128\n","Epoch 0072: train loss 4.6133, valid loss 4.6283\n","Epoch 0073: train loss 4.6110, valid loss 4.6208\n","Epoch 0074: train loss 4.6055, valid loss 4.6142\n","Epoch 0075: train loss 4.5988, valid loss 4.6263\n","Epoch 0076: train loss 4.6065, valid loss 4.6323\n","Epoch 0077: train loss 4.5966, valid loss 4.5975\n","Epoch 0078: train loss 4.5958, valid loss 4.6323\n","Epoch 0079: train loss 4.5901, valid loss 4.6136\n","Epoch 0080: train loss 4.5876, valid loss 4.6335\n","Epoch 0081: train loss 4.5779, valid loss 4.6044\n","Epoch 0082: train loss 4.5880, valid loss 4.5910\n","Epoch 0083: train loss 4.5724, valid loss 4.5847\n","Epoch 0084: train loss 4.5695, valid loss 4.6274\n","Epoch 0085: train loss 4.5752, valid loss 4.6013\n","Epoch 0086: train loss 4.5690, valid loss 4.5796\n","Epoch 0087: train loss 4.5578, valid loss 4.5645\n","Epoch 0088: train loss 4.5635, valid loss 4.5891\n","Epoch 0089: train loss 4.5611, valid loss 4.5889\n","Epoch 0090: train loss 4.5507, valid loss 4.5634\n","Epoch 0091: train loss 4.5438, valid loss 4.5741\n","Epoch 0092: train loss 4.5455, valid loss 4.5732\n","Epoch 0093: train loss 4.5390, valid loss 4.5769\n","Epoch 0094: train loss 4.5393, valid loss 4.5573\n","Epoch 0095: train loss 4.5488, valid loss 4.5959\n","Epoch 0096: train loss 4.5385, valid loss 4.5587\n","Epoch 0097: train loss 4.5311, valid loss 4.5698\n","Epoch 0098: train loss 4.5323, valid loss 4.5269\n","Epoch 0099: train loss 4.5212, valid loss 4.5583\n","Epoch 0100: train loss 4.5250, valid loss 4.5508\n","Finished Training.\n"]}]},{"cell_type":"markdown","source":["## LSTM Variational AutoEncoder"],"metadata":{"id":"ZE3BsmeRyQnn"}},{"cell_type":"code","source":["train_losses_lstm_vae, val_losses_lstm_vae = train_model(model_lstm_vae, train_loader, val_loader, optimizer_lstm_vae, loss_function_lstm_vae, scheduler_lstm_vae, True, num_epochs=100, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YqN6z7M0AdS","executionInfo":{"status":"ok","timestamp":1746390216252,"user_tz":-120,"elapsed":102040,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"5c7a82fd-bbe0-468d-eac0-7cca1234f4d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-17-eb483228514d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","<ipython-input-17-eb483228514d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0001: train loss 111.0180, valid loss 69.1832\n","Epoch 0002: train loss 65.4164, valid loss 64.8201\n","Epoch 0003: train loss 63.8878, valid loss 64.0853\n","Epoch 0004: train loss 63.6279, valid loss 62.9051\n","Epoch 0005: train loss 63.3731, valid loss 63.9493\n","Epoch 0006: train loss 63.6658, valid loss 62.9409\n","Epoch 0007: train loss 62.6779, valid loss 63.1970\n","Epoch 0008: train loss 62.4767, valid loss 64.7848\n","Epoch 0009: train loss 62.5802, valid loss 62.3761\n","Epoch 0010: train loss 62.2796, valid loss 63.7612\n","Epoch 0011: train loss 62.2403, valid loss 63.3894\n","Epoch 0012: train loss 62.0063, valid loss 62.4541\n","Epoch 0013: train loss 62.1138, valid loss 62.4695\n","Epoch 0014: train loss 62.0534, valid loss 62.6712\n","Epoch 0015: train loss 61.9204, valid loss 63.2243\n","Epoch 0016: train loss 60.9410, valid loss 61.2454\n","Epoch 0017: train loss 60.5379, valid loss 61.3935\n","Epoch 0018: train loss 60.5722, valid loss 61.5015\n","Epoch 0019: train loss 60.9329, valid loss 61.4786\n","Epoch 0020: train loss 60.8432, valid loss 61.5583\n","Epoch 0021: train loss 60.5280, valid loss 61.0274\n","Epoch 0022: train loss 60.7839, valid loss 61.2630\n","Epoch 0023: train loss 60.9061, valid loss 61.4171\n","Epoch 0024: train loss 60.6690, valid loss 61.6399\n","Epoch 0025: train loss 60.5352, valid loss 61.4201\n","Epoch 0026: train loss 60.7120, valid loss 61.6371\n","Epoch 0027: train loss 61.0044, valid loss 61.2824\n","Epoch 0028: train loss 60.5359, valid loss 61.1743\n","Epoch 0029: train loss 60.6805, valid loss 61.9611\n","Epoch 0030: train loss 60.5104, valid loss 61.3819\n","Epoch 0031: train loss 60.3165, valid loss 61.2541\n","Early stopping triggered.\n","Finished Training.\n"]}]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"wIrIycCByUJb"}},{"cell_type":"code","source":["def evaluate_model(model, variational, test_loader, device, loss_fn, percentile_threshold=90):\n","    model.eval()\n","    anomaly_scores = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            batch = torch.tensor(batch, dtype=torch.float32).to(device)\n","\n","            batch_scores = []\n","            for i in range(batch.shape[0]):  # Iterate through each sequence in the batch\n","                sequence = batch[i].unsqueeze(0)  # Shape: (1, seq_len, features)\n","                if variational:\n","                    recon_sequence, mean, logvar = model(sequence)\n","                    loss = loss_fn(recon_sequence, sequence, mean, logvar)\n","                else:\n","                    recon_sequence = model(sequence)\n","                    loss = loss_fn(sequence, recon_sequence)\n","                batch_scores.append(loss.item())\n","\n","            anomaly_scores.extend(batch_scores)\n","\n","    # Calculate threshold and identify anomalies\n","    threshold = np.percentile(anomaly_scores, percentile_threshold)\n","    anomaly_indices = [i for i, score in enumerate(anomaly_scores) if score > threshold]\n","    return anomaly_indices"],"metadata":{"id":"GXPXM9Lt1jDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_f1_score(anomaly_indices, true_anomalies):\n","    # Create a binary array representing predicted anomalies\n","    predicted_anomalies = np.zeros_like(true_anomalies)\n","    for index in anomaly_indices:\n","        if index < len(predicted_anomalies):  # Check index bounds\n","          predicted_anomalies[index] = 1\n","\n","    # Calculate the F1 score\n","    f1 = f1_score(true_anomalies, predicted_anomalies)\n","    return f1, predicted_anomalies"],"metadata":{"id":"NkhSBc1z2Pls"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AutoEncoder"],"metadata":{"id":"xjRGdTxNyap6"}},{"cell_type":"code","source":["anomalies_ae = evaluate_model(model_ae, False, test_loader, device, loss_function_ae, 90)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqMBuOYN1mxI","executionInfo":{"status":"ok","timestamp":1746390224170,"user_tz":-120,"elapsed":7897,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"ff1be548-cb1c-4bcb-b4fc-1b55bb82d1dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-22-6ed720cb0935>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]}]},{"cell_type":"code","source":["f1_ae, predicted_anomalies_ae = calculate_f1_score(anomalies_ae, true_anomalies)\n","print(f\"F1 Score: {f1_ae}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMR4BtWC2Tyr","executionInfo":{"status":"ok","timestamp":1746390224171,"user_tz":-120,"elapsed":13,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"3fa848a2-c631-43c8-9f15-5f4f59ee7292"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.5784437624119877\n"]}]},{"cell_type":"code","source":["print(classification_report(true_anomalies, predicted_anomalies_ae))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kW1_UDGz2UNZ","executionInfo":{"status":"ok","timestamp":1746390224172,"user_tz":-120,"elapsed":8,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"34a44d8c-3a3f-4549-8e94-184ebf8c18d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.95      0.95     25785\n","           1       0.56      0.59      0.58      2694\n","\n","    accuracy                           0.92     28479\n","   macro avg       0.76      0.77      0.77     28479\n","weighted avg       0.92      0.92      0.92     28479\n","\n"]}]},{"cell_type":"code","source":["print(confusion_matrix(true_anomalies, predicted_anomalies_ae))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBMQeHrW2VsX","executionInfo":{"status":"ok","timestamp":1746390224172,"user_tz":-120,"elapsed":5,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"a94728d9-b4c9-4fdd-c9b1-69d3987fe63a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[24542  1243]\n"," [ 1092  1602]]\n"]}]},{"cell_type":"markdown","source":["## LSTM AutoEncoder"],"metadata":{"id":"QWhkcjKQybqC"}},{"cell_type":"code","source":["anomalies_lstm_ae = evaluate_model(model_lstm_ae, False, test_loader, device, loss_function_lstm_ae, 90)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tmu1tVlv2FPL","executionInfo":{"status":"ok","timestamp":1746390244741,"user_tz":-120,"elapsed":20572,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"f1f42057-3540-43d3-b030-0f9f74e40d44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-22-6ed720cb0935>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]}]},{"cell_type":"code","source":["f1_lstm_ae, predicted_anomalies_lstm_ae = calculate_f1_score(anomalies_lstm_ae, true_anomalies)\n","print(f\"F1 Score: {f1_lstm_ae}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4pkCHJg2Y3f","executionInfo":{"status":"ok","timestamp":1746390244744,"user_tz":-120,"elapsed":22,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"859e0d7f-68d7-4d42-84c1-97b393a84b57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.5542516699765301\n"]}]},{"cell_type":"code","source":["print(classification_report(true_anomalies, predicted_anomalies_lstm_ae))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUx_V7Oa2Y5W","executionInfo":{"status":"ok","timestamp":1746390244744,"user_tz":-120,"elapsed":12,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"5209d46d-32c0-4415-cf6b-82d9659f9565"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.95      0.95     25785\n","           1       0.54      0.57      0.55      2694\n","\n","    accuracy                           0.91     28479\n","   macro avg       0.75      0.76      0.75     28479\n","weighted avg       0.92      0.91      0.91     28479\n","\n"]}]},{"cell_type":"code","source":["print(confusion_matrix(true_anomalies, predicted_anomalies_lstm_ae))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NX7Lv5O32Y7g","executionInfo":{"status":"ok","timestamp":1746390244745,"user_tz":-120,"elapsed":8,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"7045e1cd-7595-44aa-8a67-ee8796260f65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[24475  1310]\n"," [ 1159  1535]]\n"]}]},{"cell_type":"markdown","source":["## LSTM Variational AutoEncoder"],"metadata":{"id":"g2_TWpfGycvh"}},{"cell_type":"code","source":["anomalies_lstm_vae = evaluate_model(model_lstm_vae, True, test_loader, device, loss_function_lstm_vae, 90)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00oIIYKJ2LIt","executionInfo":{"status":"ok","timestamp":1746390272150,"user_tz":-120,"elapsed":27410,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"b1458e9a-0f54-468c-dfef-138bfd0becb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-22-6ed720cb0935>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  batch = torch.tensor(batch, dtype=torch.float32).to(device)\n"]}]},{"cell_type":"code","source":["f1_lstm_vae, predicted_anomalies_lstm_vae = calculate_f1_score(anomalies_lstm_vae, true_anomalies)\n","print(f\"F1 Score: {f1_lstm_vae}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBCeC4V42i9s","executionInfo":{"status":"ok","timestamp":1746390272172,"user_tz":-120,"elapsed":16,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"b4aa2175-abb2-438e-b973-8d2286719f33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.6141902870554252\n"]}]},{"cell_type":"code","source":["print(classification_report(true_anomalies, predicted_anomalies_lstm_vae))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P56-eb9U2i_7","executionInfo":{"status":"ok","timestamp":1746390272174,"user_tz":-120,"elapsed":10,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"4be5bc13-53d4-48de-d1e6-9f8b9cd35175"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.96      0.96     25785\n","           1       0.60      0.63      0.61      2694\n","\n","    accuracy                           0.92     28479\n","   macro avg       0.78      0.79      0.79     28479\n","weighted avg       0.93      0.92      0.93     28479\n","\n"]}]},{"cell_type":"code","source":["print(confusion_matrix(true_anomalies, predicted_anomalies_lstm_vae))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYzVR-EL2jCH","executionInfo":{"status":"ok","timestamp":1746390272175,"user_tz":-120,"elapsed":7,"user":{"displayName":"Henrik Berényi","userId":"03670554175875550861"}},"outputId":"a9d52ee8-2314-40ee-b359-e1f0ce8372f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[24641  1144]\n"," [  993  1701]]\n"]}]}]}